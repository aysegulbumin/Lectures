{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4 - Linear Regression cont., Generalization & Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Curve Fitting continued...\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 4 - Model Fitting</b> \n",
    "\n",
    "Also referred as training the model.\n",
    "\n",
    "We *fit* the polynomial function model such that the *objective function* $J(\\mathbf{w})$ is minimized, i.e. we *optimize* the following error function\n",
    "\n",
    "\\begin{align}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} \\left(\\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right)^T \\left(\\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right) \\\\\n",
    "&= \\frac{1}{2} \\left\\Vert \\mathbf{X}\\mathbf{w} - \\mathbf{t} \\right\\Vert_2^2\n",
    "\\end{align}\n",
    "\n",
    "* This function is called the **least squares error** objective function.\n",
    "\n",
    "The optimization function is then:\n",
    "$$\\arg_{\\mathbf{w}}\\min J(\\mathbf{w})$$ \n",
    "</div>\n",
    "\n",
    "* So, we want $J(\\mathbf{w})$ to be small. What is the set of parameters $\\mathbf{w}$ that minimize the objective function $J(\\mathbf{w})$?\n",
    "\n",
    "* What do you mean by **optimize** $J(\\mathbf{w})$? **How do you find $\\mathbf{w}$?**\n",
    "\n",
    "* To do that, we **take the derivative of $J(\\mathbf{w})$ with respect to the parameters $\\mathbf{w}$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The derivative of the scalar $J(\\mathbf{w})$ with respect to the vector $\\mathbf{w}=[w_0,w_1,\\dots,w_M]^T$ is a **vector**, and it corresponds to take the derivative of $J(\\mathbf{w})$ with respect to every element in $\\mathbf{w}$:\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} = \\left[ \\frac{\\partial J(\\mathbf{w})}{\\partial w_0},  \\frac{\\partial J(\\mathbf{w})}{\\partial w_1}, \\ldots,  \\frac{\\partial J(\\mathbf{w})}{\\partial w_M} \\right]^T$$\n",
    "\n",
    "* If we rewrite the objective function as:\n",
    "\\begin{align}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} \\left( \\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right)^T\\left( \\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right) \\text{, apply the transpose} \\\\\n",
    "& = \\frac{1}{2} \\left( \\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)\\left( \\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right)\\text{, apply the distributive property}  \\\\\n",
    "& = \\frac{1}{2} \\left(\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T \\mathbf{t} - \\mathbf{t}^T\\mathbf{X}\\mathbf{w} + \\mathbf{t}^T\\mathbf{t}\\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "* Solving for $\\mathbf{w}$, we find:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} &= 0 \\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\left[\\frac{1}{2} \\left(\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T \\mathbf{t} - \\mathbf{t}^T\\mathbf{X}\\mathbf{w} + \\mathbf{t}^T\\mathbf{t}\\right) \\right] &= 0 \\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\left[ \\left(\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T \\mathbf{t} - \\mathbf{t}^T\\mathbf{X}\\mathbf{w} + \\mathbf{t}^T\\mathbf{t}\\right) \\right] &= 0\\text{, apply product rule for the first term} \\\\\n",
    "(\\mathbf{X}^T\\mathbf{X}\\mathbf{w})^T + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} - (\\mathbf{X}^T \\mathbf{t})^T - \\mathbf{t}^T\\mathbf{X} &=0 \\\\\n",
    "\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} - \\mathbf{t}^T\\mathbf{X} - \\mathbf{t}^T\\mathbf{X} &= 0\\\\\n",
    "2 \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} &= 2 \\mathbf{t}^T\\mathbf{X} \\\\\n",
    "(\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X})^T &= (\\mathbf{t}^T\\mathbf{X})^T\\text{, apply transpose on both sides} \\\\\n",
    "\\mathbf{X}^T\\mathbf{X}\\mathbf{w} &= \\mathbf{X}^T\\mathbf{t} \\\\\n",
    "\\mathbf{w} &= \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n",
    "\\end{align}\n",
    "\n",
    "* This gives us the **optimal set of parameters** $\\mathbf{w}^*$ that minimize the objective function $J(\\mathbf{w})$ for the training data $\\{X,\\mathbf{t}\\}_{i=1}^N$, and so,\n",
    "\n",
    "$$\\mathbf{w}^* = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}$$\n",
    "\n",
    "* So, for all pair of input data points $(x_i,t_i)$, we can write the estimated value of $t_i$ as:\n",
    "\n",
    "$$\\hat{t}_i = y_i = {\\mathbf{w}^*}^T\\phi(x_i)$$\n",
    "\n",
    "For all data points in matrix form $\\mathbf{X}$, we can write:\n",
    "\n",
    "$$\\hat{\\mathbf{t}} = \\mathbf{y} = X\\mathbf{w}$$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Here is a quick video review of inverse matrices: [Season 1 Episode 7 of 3Blue1Brown Series](https://www.youtube.com/watch?v=uQhTuRlWMxw&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=8&t=33s)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data**\n",
    "\n",
    "After the model is trained (i.e. complete optimization of error function using the training labeled data), the **goal** is to *predict* the output values to *new*, unseen and unlabeled test data.\n",
    "\n",
    "The steps in the test data are:\n",
    "* **Step 1:** Extract (the same) features\n",
    "* **Step 2:** Run through the trained model using the optimal set of parameters $\\mathbf{w}^*$ to find the output value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Implementation\n",
    "\n",
    "**What can you control?** \n",
    "\n",
    "<!-- * Training data -- in some cases we can't control the training data,\n",
    "* Model order, and\n",
    "* Feature vectors or *basis functions* -->\n",
    "\n",
    "How would you implement linear regression using polynomial features?\n",
    " * Let's see with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Suppose Input Data is sampled from a (noisy) sine curve \n",
    "\n",
    "Suppose our data comes from a noisy sinusoidal: $t = \\sin(2\\pi x) + \\epsilon$ where $\\epsilon$ is a (univariate) Gaussian zero-mean random noise. \n",
    "\n",
    "* The univariate Gaussian Distribution is defined as:\n",
    "\t\\begin{eqnarray}\n",
    "\t\t\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{(2\\pi \\sigma^2)^{1/2}} \\exp\\left\\{ - \\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2\\right\\}\n",
    "\t\\end{eqnarray}\n",
    "\n",
    "    where $\\mu$ is the mean and $\\sigma^2$ is the variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the noise is zero-mean Gaussian distributed, it is like we are saying there is a Gaussian around the true curve: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\t\t t = y + \\epsilon\\\\\n",
    "\t\t \\epsilon = t - y\n",
    "\t \\end{eqnarray}\n",
    "\t where\n",
    "\t \\begin{eqnarray}\n",
    "\t \t\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    " \t \\end{eqnarray}\n",
    " \t thus\n",
    " \t \\begin{eqnarray}\n",
    " \t \t\\mathcal{N}(t-y|0,1) &\\propto& \\exp\\left\\{ -\\frac{1}{2} \\frac{(t-y-0)^2}{1^2} \\right\\}\\\\\n",
    " \t \t&=& \\exp\\left\\{ -\\frac{1}{2} (t-y)^2 \\right\\}\\\\\n",
    " \t \t&=&  \\exp\\left\\{ -J(\\mathbf{w}) \\right\\}\n",
    " \t\\end{eqnarray}\n",
    "\n",
    "* The **least squares** objective function, $J(\\mathbf{w})$, assumes Gaussian noise. \n",
    "\n",
    "    * Another way to look at it: the desired values, $t$, are distributed according to a Gaussian distribution with mean $y$!\n",
    "\n",
    "Let's generate data from the *true* underlying function (which, in practice, we would not know)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisySinusoidalData(N, a, b, gVar):\n",
    "    '''NoisySinusoidalData(N, a, b, gVar): Generates N data points in the range [a,b] \n",
    "    sampled from a sin(2*pi*x) with additive zero-mean Gaussian random noise with standard deviation gVar'''\n",
    "    x = np.linspace(a,b,N) # N input samples, evenly spaced numbers between [a,b]\n",
    "    noise = np.random.normal(0,gVar,N) # draw N sampled from a univariate Gaussian distribution with mean 0, gVar standard deviation and N data points\n",
    "    t = np.sin(2*np.pi*x) + noise # desired values, noisy sinusoidal \n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input samples and desired values\n",
    "\n",
    "N = 40 # number of training data samples\n",
    "Ntest = 10 # number test data samples\n",
    "a, b = [0,1] # data samples interval\n",
    "gVar_train = 0.5 # standard deviation of the zero-mean Gaussian noise observed in the training samples\n",
    "gVar_test = 1 # standard deviation of the zero-mean Gaussian noise observed in the testing samples\n",
    "x1, t1 = NoisySinusoidalData(N, a, b, gVar_train) # Training Data - Noisy sinusoidal\n",
    "x2, t2 = NoisySinusoidalData(N, a, b, 0) # True Sinusoidal - in practice, we don't the true function\n",
    "x3, t3 = NoisySinusoidalData(Ntest, a, b, gVar_test) # Test Data - Noisy sinusoidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x1, t1, 'bo', label = 'Training Data')\n",
    "plt.plot(x2, t2, 'g', label = 'True Sinusoidal')\n",
    "plt.plot(x3, t3, 'ro', label = 'Test Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x')\n",
    "plt.ylabel('Desired Values, t');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the data using the *polynomial regression* model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(x,t,M):\n",
    "    '''PolynomialRegression(x,t,M): Fit a polynomial of order M to the data input data x and desire values t'''\t\n",
    "    # To be completed in class\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Order\n",
    "M = 13\n",
    "\n",
    "# Find the parameters that fit the noisy sinusoidal\n",
    "w, y, error = PolynomialRegression(x1,t1,M) \n",
    "\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x1,y,'r', label = 'Estimated Polynomial')\n",
    "plt.plot(x2,t2,'g', label = 'True Function')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Data Samples, x\")\n",
    "plt.ylabel(\"Desired Values, t\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How do the weights look like? - Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well does this trained model **generalizes** to the **test data**, to which we do **not** have labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed in class\n",
    "\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x1,y,'r', label = 'Estimated Polynomial')\n",
    "plt.plot(x2,t2,'g', label = 'True Function')\n",
    "plt.plot(x3,t3,'ro', label = 'Test Data')\n",
    "plt.plot(x3,y_test,'k', label = 'Test Predictions')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Data Samples, x\")\n",
    "plt.ylabel(\"Desired Values, t\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What happens when the test points fall outside the range of what the model has *learned*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4, t4 = NoisySinusoidalData(10, 0.5, 1.5, 0.1)\n",
    "x2, t2 = NoisySinusoidalData(N, 0, 1.5, 0)\n",
    "\n",
    "plt.plot(x1, t1, 'bo', label = 'Training Data')\n",
    "plt.plot(x2, t2, 'g', label = 'True Sinusoidal')\n",
    "plt.plot(x4, t4, 'ro', label = 'Test Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x')\n",
    "plt.ylabel('Desired Values, t');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed in class\n",
    "\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x1,y,'r', label = 'Estimated Polynomial')\n",
    "plt.plot(x2,t2,'g', label = 'True Function')\n",
    "plt.plot(x3,t3,'ro', label = 'Test Data')\n",
    "plt.plot(x4,y_test,'k', label = 'Test Predictions')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Data Samples, x\")\n",
    "plt.ylabel(\"Desired Values, t\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How do we select the *best* model order? - Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Least Squares error for different model orders M\n",
    "\n",
    "J_train = []\n",
    "J_test = []\n",
    "for M in range(15):\n",
    "    w, y, error_training = PolynomialRegression(x1,t1,M)\n",
    "    X_test = np.array([x3**m for m in range(M+1)]).T\n",
    "    y_test = X_test@w  \n",
    "    error_Test = y_test - t3\n",
    "    J_train+=[np.sum(error_training**2)/2]\n",
    "    J_test+= [np.sum(error_Test**2)]\n",
    "\n",
    "plt.plot(J_train,'bo-', label = 'Training')\n",
    "plt.plot(J_test,'ro-', label = 'Test')\n",
    "plt.title('Cost Function')\n",
    "plt.legend()\n",
    "plt.xlabel('Model order, M')\n",
    "plt.ylabel('J(w)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using libraries\n",
    "\n",
    "We can also implement linear regression using the function ```LinearRegression``` from the module ```linear_model``` within the library ```sklearn```.\n",
    "\n",
    "* Check out ```scikit-learn``` documentation on ```LinearRegression``` model [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "* Check out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(x1[:,np.newaxis], t1)\n",
    "\n",
    "print(model.score(x1[:,np.newaxis], t1[:,np.newaxis]))\n",
    "print('The equation of the line is: y = ' + str(model.coef_[0]) + '*x + ' + str(model.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfit = model.predict(x1[:, np.newaxis])\n",
    "# yfit3 = model.predict(x3[:,np.newaxis])\n",
    "\n",
    "plt.plot(x1,t1, 'bo', label = 'Training Data')\n",
    "plt.plot(x1, yfit, 'r', label = 'Best-fit Line')\n",
    "# plt.plot(x3,t3, 'ro', label = 'Test Data')\n",
    "# plt.plot(x3, yfit3, 'k', label = 'Best-fit Line Test')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Data Samples, x\")\n",
    "plt.ylabel(\"Desired Values, t\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "M = 3\n",
    "poly = PolynomialFeatures(M, include_bias=True)\n",
    "poly.fit_transform(x1[:,np.newaxis]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "M = 3\n",
    "poly_model = make_pipeline(PolynomialFeatures(M),LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model.fit(x1[:, np.newaxis], t1)\n",
    "yfit = poly_model.predict(x1[:, np.newaxis])\n",
    "# yfit3 = poly_model.predict(x3[:,np.newaxis])\n",
    "\n",
    "plt.plot(x1,t1, 'bo', label = 'Training Data')\n",
    "plt.plot(x1, yfit, 'r', label = 'Estimated Polynomial')\n",
    "# plt.plot(x3,t3, 'ro', label = 'Test Data')\n",
    "# plt.plot(x3, yfit3, 'k', label = 'Test Predictions')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Data Samples, x\")\n",
    "plt.ylabel(\"Desired Values, t\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization\n",
    "\n",
    "In Machine Learning, we want our supervised learning model to be able to *generalize* knowledge acquired from the training stage to new unknown data. So we don't want to train the model such that it *memorizes* the training samples, nor we want to make the model so simple that is simply doesn't work. These two situations are known as **overfitting** and **underfitting** respectively.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Generalization</b> \n",
    "\n",
    "A model is said to have **generalization** abilities if it can *properly* adapt to new and unseen data that follows the same distribution as the one used to train the model.\n",
    "</div>\n",
    "\n",
    "\n",
    "## Overfitting/Overtraining & Underfitting/Undertraining\n",
    "\n",
    "After selecting our *linear model*, the only parameter we can *control* is the *model order* $M$.\n",
    "\n",
    "* As $M$ increases, there are more parameters (more elements in the vector $\\mathbf{w}$) to learn and, so, the model becomes more complex.\n",
    "\n",
    "* The order $M$ regulates the model *complexity*.\n",
    "    * In a Polynomial regresssion it also controls the richness of the feature space. \n",
    "\n",
    "As a model that is *complex*, it is more likely to *overfit* or *overtrain*.\n",
    "* This essentially means it may \"memorize\" the input training data,\n",
    "* Including all of the training data's noise!\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Overfitting</b> \n",
    "\n",
    "In model fitting, a model is said to have **overfit** to the training data, if it is *not* able to *generalize* to new and unknown data. We can observe this effect as the training error is small but the error in the test data is very large.\n",
    "\n",
    "In other words, overfitting means that the *true* underlying model of the data is not estimated/learned properly, instead the model returned a poor representation that memorized meaningless noise in the data.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Underfitting</b> \n",
    "\n",
    "In model fitting, a model is said to have **underfit** if the error in both in training and test data sets are still decreasing. \n",
    "\n",
    "An underfit model is not complex enough to model all the characteristics in the data.\n",
    "</div>\n",
    "\n",
    "<div><img src=\"figures/Under_vs_Overfitting.png\", width=\"600\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to avoid overfitting?\n",
    "\n",
    "* **Cross-validation**: tests the model's ability to predict new data that was not used in estimating the model, in order to flag problems like *overfitting* or *selection bias* and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset).\n",
    "* **More data**: As you have more and more data, it becomes more and more difficult to \"memorize\" the data and its noise. Often, more data translates to the ability to use a more complex model and avoid overfitting. Adding more data, also allows the data matrix, $\\mathbf{X}^T\\mathbf{X}$, to *explain* the entire $M$-dimensional space and prevent it from becoming *singular*.\n",
    "    * However, generally, you need exponentially more data with increases to model complexity.  So, there is a limit to how much this helps. If you have a very complex model, you need a huge training data set. \n",
    "    * (Note: A matrix is said to be *singular* if its determinant is equal to zero, this means it is **not** invertible.)\n",
    "* **Regularization**: add a penalty term to the error function to discourage learning a more complex model by forcing the model parameters to be small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>k-fold Cross-Validation</b> \n",
    "\n",
    "The goal of **cross-validation** is to test the model's ability to predict new data that was not used in estimating the model, in order to flag problems like *overfitting* or *selection bias* and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset).\n",
    "\n",
    "The technique of **S-fold cross-validation**, illustrated below for the case of $S = 4$, involves taking the available data and partitioning it into $S$ groups (in the simplest case these are of equal size). Then $S-1$ of the groups are used to train a set of models that are then evaluated on the remaining group. This procedure is then repeated for all $S$ possible choices for the held-out group, indicated in the picture below by the red blocks, and the performance scores from the S runs are then averaged.\n",
    "</div>\n",
    "\n",
    "<div><img src=\"figures/SfoldCrossValidation.png\", width=\"300\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Regularization</b> \n",
    "\n",
    "**Regularization** contrains/*regularizes* or shrinks the parameter coefficients such that they cannot take a large value. Regularization of the model parameters discourages learning a more complex, so to avoid the risk of overfitting.\n",
    "\n",
    "$$E_W(\\mathbf{w}) = \\frac{1}{2}\\mathbf{w}^T\\mathbf{w} = \\left\\Vert\\mathbf{w}\\right\\Vert_2^2$$\n",
    "\n",
    "where\n",
    "\n",
    "$$E(\\mathbf{w}) = E_D(\\mathbf{w}) + \\lambda E_W(\\mathbf{w}) = \\frac{1}{2} \\left\\Vert \\mathbf{\\Phi}\\mathbf{w} - \\mathbf{t} \\right\\Vert^2_2 + \\lambda\\frac{1}{2}\\left\\Vert\\mathbf{w}\\right\\Vert_2^2$$\n",
    "\n",
    "where $\\lambda$ is the regularization coefficient that controls the relative importance of the data-dependent error $E_D(\\mathbf{w})$ and the regularization term $E_W(\\mathbf{w})$.\n",
    "</div>\n",
    "\n",
    "* This particular choice of regularizer is known in the machine learning literature as *weight decay* or *ridge regularizer* because in sequential learning algorithms, it encourages weight values to decay towards zero, unless supported by the data.\n",
    "\n",
    "* It has the advantage that the error function remains a quadratic function of $\\mathbf{w}$, and so its exact minimizer can be found in closed form.\n",
    "\n",
    "* A more general regularizer is sometimes used, for which the regularized error takes the form:\n",
    "\n",
    "$$E(\\mathbf{w}) = \\frac{1}{2} \\left\\Vert \\mathbf{\\Phi}\\mathbf{w} - \\mathbf{t} \\right\\Vert^2_2 + \\lambda\\frac{1}{2}\\left|\\mathbf{w}\\right|^q$$\n",
    "\n",
    "where $q=2$ corresponds to the quadratic regularizer. \n",
    "* The case of $q = 1$ is known as the *lasso* regularizer. It has the property that if $\\lambda$ is sufficiently large, some of the coefficients $w_j$ are driven to zero, leading to a *sparse* model in which the corresponding basis\n",
    "functions play no role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = np.linspace(-2,2,1000)\n",
    "\n",
    "plt.plot(xrange,xrange**2, label = 'Ridge Regressor')\n",
    "plt.plot(xrange,np.abs(xrange), label = 'Lasso Regressor')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ridge Regularizer prefers to weight parameter elements to be non-zero.\n",
    "\n",
    "* Ridge Regularizer is highly affected by outliers.\n",
    "\n",
    "* Lasso Regularizer promotes sparsity.\n",
    "\n",
    "* Lasso Regularizer not as affected by outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Regularized Least Squares</b> \n",
    "\n",
    "We fit the linear regression model such that the *regularized* objective function $E(\\mathbf{w})$ is minimized:\n",
    "\n",
    "\\begin{align}\n",
    "& \\arg_{\\mathbf{w}}\\min E(\\mathbf{w}) \\\\ \n",
    "\\text{where } E(\\mathbf{w}) &= E_D(\\mathbf{w}) + \\frac{\\lambda}{2} E_W(\\mathbf{w}) \\\\\n",
    "&= \\frac{1}{2} \\left\\Vert \\mathbf{X}\\mathbf{w} - \\mathbf{t} \\right\\Vert^2_2 + \\lambda\\frac{1}{2}\\left\\Vert\\mathbf{w}\\right\\Vert_2^2 \\\\\n",
    "&= \\frac{1}{2} \\left(\\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right)^T\\left(\\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right) + \\lambda\\frac{1}{2} \\mathbf{w}^T\\mathbf{w} \\\\\n",
    "&= \\frac{1}{2} \\left(\\left(\\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right)^T\\left(\\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right) + \\lambda\\mathbf{w}^T\\mathbf{w}\\right)\n",
    "\\end{align}\n",
    "\n",
    "This is often referred to as the **ridge regression**.\n",
    "\n",
    "The optimal solution $\\mathbf{w}^*$ is:\n",
    "\n",
    "$$\\mathbf{w}^* = \\left(\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I}\\right)^{-1}\\mathbf{X}^T\\mathbf{t} $$\n",
    "\n",
    "where $\\mathbf{I}$ is an identity matrix of size $(M+1)\\times (M+1)$.\n",
    "</div>\n",
    "\n",
    "* We are **diagonally loading** the matrix $\\mathbf{X}^T\\mathbf{X}$ with the regularizer term $\\lambda$.\n",
    "* This is \"filling\" the feature space such that the matrix $\\mathbf{X}^T\\mathbf{X}$ becomes full rank.\n",
    "* What happens when $\\lambda \\rightarrow \\infty$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "* In code, how would you change the function ```PolynomialRegression``` created above to include the regularization term?\n",
    "\n",
    "* How could you incorporate regularizer into a ```scikit-learn``` pipeline?\n",
    "\n",
    "* How about variables normalizaion -- pre-processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "M = 3\n",
    "poly_model = make_pipeline(StandardScaler(),PolynomialFeatures(M),Ridge(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model.fit(x1[:, np.newaxis], t1)\n",
    "yfit = poly_model.predict(x1[:, np.newaxis])\n",
    "# yfit3 = poly_model.predict(x3[:,np.newaxis])\n",
    "\n",
    "plt.plot(x1,t1, 'bo', label = 'Training Data')\n",
    "plt.plot(x1, yfit, 'r', label = 'Estimated Polynomial')\n",
    "# plt.plot(x3,t3, 'ro', label = 'Test Data')\n",
    "# plt.plot(x3, yfit3, 'k', label = 'Test Predictions')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Data Samples, x\")\n",
    "plt.ylabel(\"Desired Values, t\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
