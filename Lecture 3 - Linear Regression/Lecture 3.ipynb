{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3 - Linear Regression, Generalization & Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "**Supervised Learning:** Learning a mapping from input data to desired output values given labeled training data.\n",
    "\n",
    "The typical workflow of a Supervised Learning algorithm is as follows:\n",
    "\n",
    "**Training Stage**\n",
    "1. Collect labeled training data - often the most time-consuming and expensive task.\n",
    "2. Extract features - extract *useful* features from the input (or observational) data such that they have discriminatory information in successfully mapping the desired output. \n",
    "3. Select a model - relationship between input data and desired output.\n",
    "4. Fit the model - change model parameters (*Learning Algorithm*) in order to meet some *Objective Function*.\n",
    "\n",
    "<div><img src=\"figures/SupervisedLearning.png\", width=\"800\"><!div>\n",
    "\n",
    "**Testing Stage**\n",
    "1. Given unlabeled test data\n",
    "2. Extract (the same) features\n",
    "3. Run the unabeled data through the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "**Linear Regression** is a form of *predictive modelling* approach to characterize the relationship between some collection of observational input data and a continuous desired response. \n",
    "\n",
    "* A linear regression model is a linear *weighted* combination of input values.\n",
    "\n",
    "* Regression is a type of **supervised** machine learning problem.\n",
    "\n",
    "Consider the example below:\n",
    "\n",
    "* The goal is to *train* a model that takes in the silhouette images with their correspondent labels (age of the person in the silhouette) and learn a linear weighted relationship between images and weight.\n",
    "\n",
    "<div><img src=\"figures/silhoutte_regression.png\", width=\"400\"><!div>\n",
    "\n",
    "* After the model is trained, the **goal** is to be able to *predict* the desired output value of any *new* unlabeld test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Curve Fitting\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Polynomial Regression** is a type of liner regression that uses a special set of *features* - polynomial features.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 1 - Input Space</b> \n",
    "\n",
    "Suppose we are given a training set comprising of $N$ observations of $\\mathbf{x}$, $\\mathbf{x} = \\left[x_1, x_2, \\ldots, x_N \\right]^T$, and its corresponding desired outputs $\\mathbf{t} = \\left[t_1, t_2, \\ldots, t_N\\right]^T$, where sample $x_i$ has the desired label $t_i$.\n",
    "\n",
    "So, we want to learn the *true* function mapping $f$ such that $\\mathbf{t}  = f(\\mathbf{x}, \\mathbf{w})$, where $\\mathbf{w}$ are parameters of the model.\n",
    "</div>\n",
    "\n",
    "* We generally organize data into *vectors* and *matrices*. Not only is it a common way to organize the data, but it allows us to easily apply linear algebraic operations during analysis. It also makes it much simpler when it comes to code implementation!\n",
    "    * In engineering textbooks and for this class, **vectors** are defined as *column vectors*. This is why we write $\\mathbf{x} = \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N\\end{array} \\right] = \\left[x_1, x_2, \\ldots, x_N \\right]^T$.\n",
    "\n",
    "* Note that both the training data and desired outputs can be noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra Review\n",
    "\n",
    "Let us review some linear algebra: \n",
    "\n",
    "* What is a vector?\n",
    "$$\\mathbf{x} = \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N\\end{array} \\right]$$\n",
    "\n",
    "* What is the transpose operation? \n",
    "$$\\mathbf{x}^T = \\left[  x_1,  x_2 , \\cdots , x_N \\right]$$\n",
    "\n",
    "* Vectors are often used to represent a data point (as a concatenation of all of the associated features for the data point)\n",
    "\n",
    "* Given a vector $\\mathbf{x} \\in \\mathbb{R}^N$ and a scalar value $a$, what is $a\\mathbf{x}$?  What does this operation do geometrically?\n",
    "\n",
    "* Given $\\mathbf{x} \\in \\mathbb{R}^N$ and $\\mathbf{y} \\in \\mathbb{R}^N$, what is $\\mathbf{x} + \\mathbf{y}$?  What is the geometric interpretation? \n",
    "\n",
    "* Given $\\mathbf{x} \\in \\mathbb{R}^N$ and $\\mathbf{y} \\in \\mathbb{R}^N$, what is $\\mathbf{x} - \\mathbf{y}$? What is the geometric interpretation? \n",
    "\n",
    "* An important operation is the *inner product*:\n",
    "$$\\mathbf{x}^T\\mathbf{y} = \\mathbf{y}^T\\mathbf{x} = \\sum_{i=1}^N x_iy_i$$\n",
    "\n",
    "* What is the *outer product*? \n",
    "$$\\mathbf{x}\\mathbf{y}^T = \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N\\end{array} \\right] \\left[ \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N\\end{array} \\right]^T = \\left[\\begin{array}{cccc} x_1y_1 & x_1y_2 & \\cdots & x_1y_N \\\\ x_2y_1 & x_2y_2 & \\cdots & x_2y_N \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_Ny_1 & x_Ny_2 & \\cdots & x_Ny_N \\end{array} \\right]$$\n",
    "\n",
    "* What is the L-p norm of a vector $\\mathbf{z} = \\left[ z_1,  z_2 , \\cdots , z_N \\right]^T$?\n",
    "\n",
    "$$\\left\\Vert \\mathbf{z} \\right\\Vert_p = \\left(z_1^p + z_2^p + \\cdots + z_N^p\\right)^{\\frac{1}{p}}$$\n",
    "\n",
    "For example, the **squared** L2-norm of a vector $\\mathbf{z}$ is $\\left\\Vert \\mathbf{z} \\right\\Vert_p^2 = \\left(\\left(z_1^2 + z_2^2 + \\cdots + z_N^2\\right)^{\\frac{1}{2}}\\right)^2 = z_1^2 + z_2^2 + \\cdots + z_N^2 = \\mathbf{z}^T\\mathbf{z}$\n",
    "\n",
    "* Note the notation:  scalar values are unbolded (e.g., $N$, $x$), vectors are lower case and bolded (e.g., $\\mathbf{x}$), arrays/matrices are uppercase and bolded (e.g., $A \\in \\mathbb{R}^{d \\times N}$), vectors are generally assumed to be column vectors.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Here is a quick video review of vectors: [Season 1 Episode 1 of 3Blue1Brown Series](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n",
    "\n",
    "Here is a quick video review of matrix multiplication: [Season 1 Episode 4 of 3Blue1Brown Series](https://www.youtube.com/watch?v=XkY2DOUCWMU&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=4)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Curve Fitting cont.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 2 - Feature Extraction</b> \n",
    "\n",
    "For the polynomial regression problem, let's consider *polynomial features* for each data point $x_i$. Let's say we can find these features using a **basis function**, $\\phi(\\mathbf{x})$. In the *polynomial regression* example, let's consider $\\phi(x_i) = \\left[\\begin{array}{ccccc} x_{i}^{0} & x_{i}^{1} & x_{i}^{2} & \\cdots & x_{i}^{M}\\end{array}\\right]^T$, where $x_i^M$ is the $M^{th}$-power of $x_i$.\n",
    "</div>\n",
    "\n",
    "* Other features can be extracted.\n",
    "\n",
    "* For all data observations $\\{x_i\\}_{i=1}^N$ and using the feature space defined as $\\phi(x_i) = \\left[\\begin{array}{ccccc} x_{i}^{0} & x_{i}^{1} & x_{i}^{2} & \\cdots & x_{i}^{M}\\end{array}\\right]^T$, we can write the input data in a *matrix* form as:\n",
    "\n",
    "$$\\mathbf{X} =\\left[\\begin{array}{c} \\phi(x_1)^T \\\\ \\phi(x_2)^T \\\\ \\vdots \\\\ \\phi(x_N)^T \\end{array}\\right]  = \\left[\\begin{array}{ccccc}\n",
    "1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{M}\\\\\n",
    "1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{M}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{M}\n",
    "\\end{array}\\right] \\in \\mathbb{R}^{N\\times (M+1)}$$\n",
    "\n",
    "where each row is a feature representation of a data point $x_i$.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Feature Space</b> \n",
    "\n",
    "The set of features drawn by the transformation \n",
    "\n",
    "\\begin{align}\n",
    "\\phi: \\mathbb{R}^D & \\rightarrow \\mathbb{R}^M \\\\\n",
    "\\mathbf{x} & \\rightarrow \\left[\\begin{array}{ccccc} x_{i}^{0} & x_{i}^{1} & x_{i}^{2} & \\cdots & x_{i}^{M}\\end{array}\\right]^T\n",
    "\\end{align}\n",
    "is often called the **feature space**.\n",
    "When we write a linear regression with respect to a set of basis functions, the regression model is linear in the *feature space*.\n",
    "\n",
    "$M$ is dimensionality of the feature space and is often called the *model order*.\n",
    "</div>\n",
    "\n",
    "* Now, we want to find the mapping from the feature input data $\\mathbf{X}$ to the desired output values $\\mathbf{t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the data actually comes from some **unknown hidden function**, that takes in the data points $\\mathbf{x}$ with some parameters $\\mathbf{w}$ and produces the desired values $\\mathbf{t}$, i.e. $\\mathbf{t} = f(\\mathbf{x},\\mathbf{w})$.\n",
    "* We do not know anything about the function $f$. If we knew the hidden function, we would not need to learn the *mapping* - we would already know it. However, since we do not know the true underlying function, we need to do our best to estimate from the examples of input-output pairs that we have.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 3 - Model Selection or Mapping</b> \n",
    "\n",
    "Let's assume that the desired output values are a *linear combination* of the feature input space, i.e., the **polynomial function**\n",
    "\n",
    "$$t \\sim y(x,\\mathbf{w}) = w_0x^0 + w_1x^1 + w_2x^2+\\cdots+w_Mx^M = \\sum_{j=0}^M w_jx^j$$\n",
    "</div>\n",
    "\n",
    "* This means that for every paired training data point $\\{x_i, t_i\\}_{i=1}^N$, we can model the output value as \n",
    "\n",
    "$$t_i \\sim y(x_i,\\mathbf{w}) = w_0x_i^0 + w_1x_i^1 + w_2x_i^2+\\cdots+w_Mx_i^M $$\n",
    "\n",
    "* Although the polynomial function $y(x,\\mathbf{w})$ is a nonlinear function of $x$, it is a linear function of the coefficients $\\mathbf{w}$. Functions, such as the polynomial, which are linear in the unknown parameters have important properties and are called *linear models*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the coefficients $\\mathbf{w}$ will be determined by *fitting* the polynomial to the training data. \n",
    "\n",
    "This can be done by minimizing an **error function** (also defined as **cost function**, **objective function**, or **loss function**) that measures the *misfit* between the function $y(x,\\mathbf{w})$, for any given value of $\\mathbf{w}$, and the training set data points $\\{x_i,t_i\\}_{i=1}^N$.\n",
    "\n",
    "* What is the model's *objective* or goal?\n",
    "\n",
    "<div><img src=\"figures/LeastSquares.png\", width=\"300\"><!div>\n",
    "\n",
    "One simple choice for fitting the model is to consider the error function given by the sum of the squares of the errors between the predictions $y(x_i,\\mathbf{w})$ for each data point $x_i$ and the corresponding target values $t_i$, so that we minimize\n",
    "\n",
    "\\begin{align*} J(\\mathbf{w}) &= \\frac{1}{2} \\sum_{n=1}^N \\left(y(x_n,\\mathbf{w}) - t_n\\right)^2 \\\\\n",
    "&= \\frac{1}{2} \\sum_{n=1}^N \\left(\\sum_{j=0}^M w_jx_n^j -t_n \\right)^2\n",
    "\\end{align*}\n",
    "\n",
    "* This error function is minimizing the (Euclidean) *distance* of every point to the curve.\n",
    "\n",
    "* **What other/s objective function can we use?**\n",
    "\n",
    "* We can write the error function compactly in matrix/vector form:\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2}\\left\\Vert y(\\mathbf{x},\\mathbf{w}) - \\mathbf{t} \\right\\Vert^2_2 \\\\\n",
    "&= \\frac{1}{2} \\left\\Vert \\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right\\Vert^2_2\\\\\n",
    "&= \\frac{1}{2} \\left(\\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right)^T \\left(\\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right)\\\\\n",
    "\\text{where: } & \\mathbf{X} = \\left[\\begin{array}{ccccc}\n",
    "1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{M}\\\\\n",
    "1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{M}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{M}\n",
    "\\end{array}\\right], \\mathbf{w} =  \\left[\\begin{array}{c}\n",
    "w_{0}\\\\\n",
    "w_{1}\\\\\n",
    "\\vdots\\\\\n",
    "w_{M}\n",
    "\\end{array}\\right], \\text{and }  \\mathbf{t} = \\left[\\begin{array}{c}\n",
    "t_{1}\\\\\n",
    "t_{2}\\\\\n",
    "\\vdots\\\\\n",
    "t_{N}\n",
    "\\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 4 - Model Fitting</b> \n",
    "\n",
    "Also referred as training the model.\n",
    "\n",
    "We *fit* the polynomial function model such that the *objective function* $J(\\mathbf{w})$ is minimized, i.e. we *optimize* the following error function\n",
    "\n",
    "\\begin{align}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} \\left(\\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right)^T \\left(\\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right) \\\\\n",
    "&= \\frac{1}{2} \\left\\Vert \\mathbf{X}\\mathbf{w} - \\mathbf{t} \\right\\Vert_2^2\n",
    "\\end{align}\n",
    "\n",
    "* This function is called the **least squares error** objective function.\n",
    "\n",
    "The optimization function is then:\n",
    "$$\\arg_{\\mathbf{w}}\\min J(\\mathbf{w})$$ \n",
    "</div>\n",
    "\n",
    "* So, we want $J(\\mathbf{w})$ to be small. What is the set of parameters $\\mathbf{w}$ that minimize the objective function $J(\\mathbf{w})$?\n",
    "\n",
    "* What do you mean by **optimize** $J(\\mathbf{w})$? **How do you find $\\mathbf{w}$?**\n",
    "\n",
    "<!-- * To do that, we **take the derivative of $J(\\mathbf{w})$ with respect to the parameters $\\mathbf{w}$**.\n",
    "\n",
    "* How do you take the derivative of a *scalar*, such as $J(\\mathbf{w})$, with respect to a vector, such as $\\mathbf{w}$?\n",
    "\n",
    "    * What is the derivative of a scalar with respect to a vector? www.wooclap.com/RBLGOK --> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The derivative of the scalar $J(\\mathbf{w})$ with respect to the vector $\\mathbf{w}=[w_0,w_1,\\dots,w_M]^T$ is a **vector**, and it corresponds to take the derivative of $E(\\mathbf{w})$ with respect to every element in $\\mathbf{w}$:\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} = \\left[ \\frac{\\partial J(\\mathbf{w})}{\\partial w_0},  \\frac{\\partial J(\\mathbf{w})}{\\partial w_1}, \\ldots,  \\frac{\\partial J(\\mathbf{w})}{\\partial w_M} \\right]^T$$\n",
    "\n",
    "* If we rewrite the objective function as:\n",
    "\\begin{align}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} \\left( \\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right)^T\\left( \\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right) \\\\\n",
    "& = \\frac{1}{2} \\left( \\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)\\left( \\mathbf{X}\\mathbf{w} - \\mathbf{t}\\right) \\\\\n",
    "& = \\frac{1}{2} \\left(\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T \\mathbf{t} - \\mathbf{t}^T\\mathbf{X}\\mathbf{w} + \\mathbf{t}^T\\mathbf{t}\\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "* Solving for $\\mathbf{w}$, we find:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} &= 0 \\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\left[\\frac{1}{2} \\left(\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T \\mathbf{t} - \\mathbf{t}^T\\mathbf{X}\\mathbf{w} + \\mathbf{t}^T\\mathbf{t}\\right) \\right] &= 0 \\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\left[ \\left(\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T \\mathbf{t} - \\mathbf{t}^T\\mathbf{X}\\mathbf{w} + \\mathbf{t}^T\\mathbf{t}\\right) \\right] &= 0\\text{, apply product rule} \\\\\n",
    "(\\mathbf{X}^T\\mathbf{X}\\mathbf{w})^T + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} - (\\mathbf{X}^T \\mathbf{t})^T - \\mathbf{t}^T\\mathbf{X} &=0 \\\\\n",
    "\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} - \\mathbf{t}^T\\mathbf{X} - \\mathbf{t}^T\\mathbf{X} &= 0\\\\\n",
    "2 \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} &= 2 \\mathbf{t}^T\\mathbf{X} \\\\\n",
    "(\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X})^T &= (\\mathbf{t}^T\\mathbf{X})^T\\text{, apply transpose on both sides} \\\\\n",
    "\\mathbf{X}^T\\mathbf{X}\\mathbf{w} &= \\mathbf{X}^T\\mathbf{t} \\\\\n",
    "\\mathbf{w} &= \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n",
    "\\end{align}\n",
    "\n",
    "* This gives us the **optimal set of parameters** $\\mathbf{w}^*$ that minimize the objective function $J(\\mathbf{w})$ for the training data $\\{X,\\mathbf{t}\\}_{i=1}^N$, and so,\n",
    "\n",
    "$$\\mathbf{w}^* = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}$$\n",
    "\n",
    "* So, for all pair of input data points $(x_i,t_i)$, we can write the estimated value of $t_i$ as:\n",
    "\n",
    "$$\\hat{t}_i = y_i = \\phi(x_i)\\mathbf{w}^*$$\n",
    "\n",
    "For all data points in matrix form $\\mathbf{X}$, we can write:\n",
    "\n",
    "$$\\hat{\\mathbf{t}} = \\mathbf{y} = X\\mathbf{w}$$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Here is a quick video review of inverse matrices: [Season 1 Episode 7 of 3Blue1Brown Series](https://www.youtube.com/watch?v=uQhTuRlWMxw&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=8&t=33s)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data**\n",
    "\n",
    "After the model is trained (i.e. complete optimization of error function using the training labeled data), the **goal** is to *predict* the output values to *new*, unseen and unlabeled test data.\n",
    "\n",
    "The steps in the test data are:\n",
    "* Step 1: Extract (the same) features\n",
    "* Step 2: Run through the trained model using the optimal set of parameters $\\mathbf{w}^*$ to find the output value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Implementation\n",
    "\n",
    "**What can you control?** \n",
    "\n",
    "<!-- * Model order -->\n",
    "<!-- * Feature vectors or *basis functions* -->\n",
    "\n",
    "How would you implement linear regression using polynomial features?\n",
    " * Let's see with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Suppose Input Data is sampled from a (noisy) sine curve \n",
    "\n",
    "Suppose our data comes from a noisy sinusoidal: $t = \\sin(2\\pi x) + \\epsilon$ where $\\epsilon$ is a (univariate) Gaussian zero-mean random noise. \n",
    "\n",
    "* The univariate Gaussian Distribution is defined as:\n",
    "\t\\begin{eqnarray}\n",
    "\t\t\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{(2\\pi \\sigma^2)^{1/2}} \\exp\\left\\{ - \\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2\\right\\}\n",
    "\t\\end{eqnarray}\n",
    "\n",
    "    where $\\mu$ is the mean and $\\sigma^2$ is the variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the noise is zero-mean Gaussian distributed, it is like we are saying there is a Gaussian around the true curve: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\t\t t = y + \\epsilon\\\\\n",
    "\t\t \\epsilon = t - y\n",
    "\t \\end{eqnarray}\n",
    "\t where\n",
    "\t \\begin{eqnarray}\n",
    "\t \t\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    " \t \\end{eqnarray}\n",
    " \t thus\n",
    " \t \\begin{eqnarray}\n",
    " \t \t\\mathcal{N}(t-y|0,1) &\\propto& \\exp\\left\\{ -\\frac{1}{2} \\frac{(t-y-0)^2}{1^2} \\right\\}\\\\\n",
    " \t \t&=& \\exp\\left\\{ -\\frac{1}{2} (t-y)^2 \\right\\}\\\\\n",
    " \t \t&=&  \\exp\\left\\{ -J(\\mathbf{w}) \\right\\}\n",
    " \t\\end{eqnarray}\n",
    "\n",
    "* The **least squares** objective function, $J(\\mathbf{w})$, assumes Gaussian noise. \n",
    "\n",
    "    * Another way to look at it: the desired values, $t$, are distributed according to a Gaussian distribution with mean $y$!\n",
    "\n",
    "Let's generate data from the *true* underlying function (which, in practice, we would not know)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisySinusoidalData(N, a, b, gVar):\n",
    "    '''NoisySinusoidalData(N, a, b, gVar): Generates N data points in the range [a,b] \n",
    "    sampled from a sin(2*pi*x) with additive zero-mean Gaussian random noise with standard deviation gVar'''\n",
    "    x = np.linspace(a,b,N) # N input samples, evenly spaced numbers between [a,b]\n",
    "    noise = np.random.normal(0,gVar,N) # draw N sampled from a univariate Gaussian distribution with mean 0, gVar standard deviation and N data points\n",
    "    t = np.sin(2*np.pi*x) + noise # desired values, noisy sinusoidal \n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input samples and desired values\n",
    "\n",
    "N = 40 # number of training data samples\n",
    "Ntest = 10 # number test data samples\n",
    "a, b = [0,1] # data samples interval\n",
    "gVar_train = 0.5 # standard deviation of the zero-mean Gaussian noise observed in the training samples\n",
    "gVar_test = 1 # standard deviation of the zero-mean Gaussian noise observed in the testing samples\n",
    "x1, t1 = NoisySinusoidalData(N, a, b, gVar_train) # Training Data - Noisy sinusoidal\n",
    "x2, t2 = NoisySinusoidalData(N, a, b, 0) # True Sinusoidal - in practice, we don't the true function\n",
    "x3, t3 = NoisySinusoidalData(Ntest, a, b, gVar_test) # Test Data - Noisy sinusoidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x1, t1, 'bo', label = 'Training Data')\n",
    "plt.plot(x2, t2, 'g', label = 'True Sinusoidal')\n",
    "plt.plot(x3, t3, 'ro', label = 'Test Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x')\n",
    "plt.ylabel('Desired Values, t');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the data using the *polynomial regression* model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(x,t,M):\n",
    "    '''PolynomialRegression(x,t,M): Fit a polynomial of order M to the data input data x and desire values t'''\t\n",
    "    # To be completed in class\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Order\n",
    "M = 13\n",
    "\n",
    "# Find the parameters that fit the noisy sinusoidal\n",
    "w, y, error = PolynomialRegression(x1,t1,M) \n",
    "\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x1,y,'r', label = 'Estimated Polynomial')\n",
    "plt.plot(x2,t2,'g', label = 'True Function')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Data Samples, x\")\n",
    "plt.ylabel(\"Desired Values, t\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How do the weights look like? - Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well does this trained model **generalizes** to the **test data**, to which we do **not** have labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed in class\n",
    "\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x1,y,'r', label = 'Estimated Polynomial')\n",
    "plt.plot(x2,t2,'g', label = 'True Function')\n",
    "plt.plot(x3,t3,'ro', label = 'Test Data')\n",
    "plt.plot(x3,y_test,'k', label = 'Test Predictions')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Data Samples, x\")\n",
    "plt.ylabel(\"Desired Values, t\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What happens when the test points fall outside the range of what the model has *learned*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4, t4 = NoisySinusoidalData(10, 0.5, 1.5, 0.1)\n",
    "x2, t2 = NoisySinusoidalData(N, 0, 1.5, 0)\n",
    "\n",
    "plt.plot(x1, t1, 'bo', label = 'Training Data')\n",
    "plt.plot(x2, t2, 'g', label = 'True Sinusoidal')\n",
    "plt.plot(x4, t4, 'ro', label = 'Test Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x')\n",
    "plt.ylabel('Desired Values, t');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed in class\n",
    "\n",
    "plt.plot(x1,t1,'bo', label='Training Data')\n",
    "plt.plot(x1,y,'r', label = 'Estimated Polynomial')\n",
    "plt.plot(x2,t2,'g', label = 'True Function')\n",
    "plt.plot(x3,t3,'ro', label = 'Test Data')\n",
    "plt.plot(x4,y_test,'k', label = 'Test Predictions')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Data Samples, x\")\n",
    "plt.ylabel(\"Desired Values, t\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How do we select the *best* model order? - Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the training into training and validation\n",
    "\n",
    "from random import sample\n",
    "\n",
    "#Shuffle the data\n",
    "temp = sample(range(N), N)\n",
    "\n",
    "#Define where to split the data\n",
    "cutPoint = round(0.8 * N) \n",
    "\n",
    "#Split the data into training and validation splits\n",
    "x_train = x1[:cutPoint]\n",
    "t_train = t1[:cutPoint]\n",
    "x_val = x1[cutPoint:]\n",
    "t_val = t1[cutPoint:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Least Squares error for different model orders M\n",
    "\n",
    "J_train = []\n",
    "J_val = []\n",
    "J_test = []\n",
    "for M in range(5):\n",
    "    w, y, error_train = PolynomialRegression(x_train,t_train,M)\n",
    "    \n",
    "    # Validation\n",
    "    X_val = np.array([x_val**m for m in range(M+1)]).T\n",
    "    y_val = X_val@w  \n",
    "    error_val = y_val - t_val\n",
    "    \n",
    "    # Test\n",
    "    X_test = np.array([x3**m for m in range(M+1)]).T\n",
    "    y_test = X_test@w  \n",
    "    error_test = y_test - t3\n",
    "    \n",
    "    # Cost function\n",
    "    J_train+=[np.sum(error_train**2)/2]\n",
    "    J_val+= [np.sum(error_val**2)/2]\n",
    "    J_test+=[np.sum(error_test**2)/2]\n",
    "\n",
    "plt.plot(J_train,'bo-', label = 'Training')\n",
    "plt.plot(J_val,'ro-', label = 'Validation')\n",
    "plt.plot(J_test,'go-', label = 'Test')\n",
    "plt.title('Objective Function')\n",
    "plt.legend()\n",
    "plt.xlabel('Model order, M')\n",
    "plt.ylabel('J(w)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using libraries\n",
    "\n",
    "We can also implement linear regression using the function ```LinearRegression``` from the module ```linear_model``` within the library ```sklearn```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(x1[:,np.newaxis], t1)\n",
    "\n",
    "print(model.score(x1[:,np.newaxis], t1[:,np.newaxis]))\n",
    "print('The equation of the line is: y = ' + str(model.coef_[0]) + '*x + ' + str(model.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfit = model.predict(x1[:, np.newaxis])\n",
    "# yfit3 = model.predict(x3[:,np.newaxis])\n",
    "\n",
    "plt.plot(x1,t1, 'bo', label = 'Training Data')\n",
    "plt.plot(x1, yfit, 'r', label = 'Best-fit Line')\n",
    "# plt.plot(x3,t3, 'ro', label = 'Test Data')\n",
    "# plt.plot(x3, yfit3, 'k', label = 'Best-fit Line Test')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Data Samples, x\")\n",
    "plt.ylabel(\"Desired Values, t\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "M = 3\n",
    "poly = PolynomialFeatures(M, include_bias=True)\n",
    "poly.fit_transform(x1[:,np.newaxis]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "M = 5\n",
    "poly_model = make_pipeline(PolynomialFeatures(M),LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model.fit(x1[:, np.newaxis], t1)\n",
    "yfit = poly_model.predict(x1[:, np.newaxis])\n",
    "# yfit3 = poly_model.predict(x3[:,np.newaxis])\n",
    "\n",
    "plt.plot(x1,t1, 'bo', label = 'Training Data')\n",
    "plt.plot(x1, yfit, 'r', label = 'Estimated Polynomial')\n",
    "# plt.plot(x3,t3, 'ro', label = 'Test Data')\n",
    "# plt.plot(x3, yfit3, 'k', label = 'Test Predictions')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Data Samples, x\")\n",
    "plt.ylabel(\"Desired Values, t\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
