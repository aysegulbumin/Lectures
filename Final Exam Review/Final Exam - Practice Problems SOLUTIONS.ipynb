{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam - Practice Problems SOLUTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Backpropagation\n",
    "\n",
    "**Use the figure below and the chain rule of derivatives to show how the weights $w_i$ can be trained through a sigmoid unit ($f(\\bullet)$ is a sigmoid) using the gradient of any smooth non-negative cost function $J=C(e)$.**\n",
    "\n",
    "<div><img src=\"net.png\", width=\"400\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img src=\"net_solution.png\", width=\"400\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Neural Network\n",
    "\n",
    "**Consider the following two-dimensional, two-class data set plotted in the figure below. Design a neural network that would obtain 100% classification accuracy on this data set. Be sure to define the network architecture and all parameters precisely (give exact numbers). Use a hard limit activation function as defined in the following equation:**\n",
    "\n",
    "$$f(x) = \\begin{cases} 0 & \\text{if } x\\leq 0 \\\\ 1 & \\text{if } x>0 \\end{cases}$$\n",
    "\n",
    "**Show and explain your work.**\n",
    "\n",
    "<div><img src=\"data.png\", width=\"300\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first determine the decision boundaries that will delimit class 0 and class 1. A picture of the linear decision boundaries can be shown in the figure below:\n",
    "\n",
    "<div><img src=\"data_solution.png\", width=\"300\"><!div>\n",
    "    \n",
    "The bottom line, $L_1$, passes through points $(10,0)$ and $(0,5)$. The top line, $L_2$, passes through points $(20,0)$ and $(0,10)$.\n",
    "    \n",
    "These lines have an approximate equation:\n",
    "\n",
    "$$L_1: y = -2x+5 \\iff 2x + y - 5 = 0$$\n",
    "    \n",
    "and \n",
    "\n",
    "$$L_2: y = -2x+10 \\iff 2x + y -10 = 0$$\n",
    "\n",
    "The network architecture needed is a 2-2-1: 2 units in the input layer, 2 units in the single hidden layer and 1 unit in the output layer.\n",
    "\n",
    "Using the equations above, we can fill in the weights and biases for the connection input-hidden layers.\n",
    "    \n",
    "The final network architecture will all the parameters is:\n",
    "\n",
    "<div><img src=\"net_problem2.png\", width=\"700\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Random Forests\n",
    "\n",
    "**What is bootstrapping and how does it relate to random forest classifiers? Be precise (use pseudo-code).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping is a sampling technique often used to create resamples of a given data set and test some hypothesis. \n",
    "\n",
    "Random forests make use of Bootstrapping. Every individual learner in a random forest, a decision tree, is trained using a Bootstrap sample to fit its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 - K-NN\n",
    "\n",
    "**This question has two parts.**\n",
    "\n",
    "1. **Given the following training data set, what is the predicted class label of the test point $[2, 2]$ if you are using a K-NN classifier with squared Euclidean distance as the distance metric and $K = 3$?**\n",
    "\n",
    "<div><img src=\"K-NN.png\", width=\"300\"><!div>\n",
    "\n",
    "2. **What is the predicted class label of the test point $[100, 100]$ if you are using a K-NN classifier with squared Euclidean distance as the distance metric and $K=3$? Given the training data, would you trust this result? Why or why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to part 1**\n",
    "\n",
    "Using Euclidean distance and $k=3$ neighbors to determine the class membership of a point in test, the class label assigned to test point $[2,2]$ will be class cross (x) as this point will have all 3 closest points belonging to class cross (x).\n",
    "\n",
    "**Solution to part 2**\n",
    "\n",
    "Similarly, for test point $[100,100]$ will be class cross (x). However, point $[100,100]$ falls outside the region of the training data, and the neighboring distances will be very large. We can use this distance information to access a confidence in the class assignment for each data point: if distance is short, then we will be confident in that decision; if the distance is large, then we will not be confident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 - Feed-forward Neural Network\n",
    "\n",
    "**Suppose you have the following network with all activation functions equal to the hard limit defined in the following equation:**\n",
    "\n",
    "$$f(x) = \\begin{cases} 0 & \\text{if }x\\leq 0 \\\\ 1 & \\text{if } x>0  \\end{cases}$$\n",
    "\n",
    "**Given a test point with the value of $[1, -2]$ what is the output of the network? Show your work.**\n",
    "\n",
    "<div><img src=\"ANN.png\", width=\"500\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the output at neuron A $y_A$, and the output at neuron B $y_B$. So the output equations for each unit in the network are given as:\n",
    "\n",
    "\\begin{align}\n",
    "y_A &= f\\left(x_1 +0.25 x_2 - 0.25\\right) \\\\\n",
    "y_B &= f\\left(0.5x_1 + 0.1 x_2 +0.9\\right)\\\\\n",
    "y_1 &= f\\left(y_A + 0.5 y_B -1.35\\right)\\\\\n",
    "y_2 &= f\\left(2y_A + 0.2\\right)\n",
    "\\end{align}\n",
    "\n",
    "where $f(x) = \\begin{cases} 0 & \\text{if }x\\leq 0 \\\\ 1 & \\text{if } x>0  \\end{cases}$.\n",
    "\n",
    "For the test point $[1,-2]$: \n",
    "\n",
    "\\begin{align}\n",
    "y_A &= f(1 -0.5 - 0.25) = 1\\\\\n",
    "y_B &= f(0.5 - 0.2 + 0.9) = 1\\\\ \n",
    "y_1 &= f(1 + 0.5 - 1.35) = 1\\\\ \n",
    "y_2 &= f(2 + 0.2) = 1\n",
    "\\end{align}\n",
    "\n",
    "The output of the network is $[1,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6 - Gradient Descent\n",
    "\n",
    "**Gradient descent is a general optimization approach where parameters of interest are iteratively updated. Does gradient descent ensure finding the global optima? Why or why not? What effect does the learning rate have on gradient descent?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the resulting solution is initialization dependent. Given an appropriate learning rate, gradient descent will move to the closest local optima.\n",
    "\n",
    "If the learning rate is too large, a step in gradient descent can jump over optima (possibly causing oscillation). A learning rate too small will take a long time to converge to a local optima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7 - Online vs Batch Learning\n",
    "\n",
    "**The following three questions are about online vs batch learning of MLPs.**\n",
    "\n",
    "1. **Briefly define Online learning and provide pseudo-code to describe how it can be implemented.**\n",
    "\n",
    "2. **Briefly define Batch learning and provide pseudo-code to describe how it can be implemented.**\n",
    "\n",
    "3. **Briefly compare and contrast online vs batch learning. What are their relative advantages/disadvantages? When would you use one over the other?**\n",
    "\n",
    "4. **Will any of these training approaches find the globally best parameter settings for a network? Why or why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A batch update uses all training data to update parameters. An epoch is equivalent to an iteration.\n",
    "\n",
    "Mini-batch uses a subset of the training data to update parameters. An epoch loops through all mini-batches to cover all training data points.\n",
    "\n",
    "Online update uses a single data point update parameters each iteration. An epoch loops through all training data points.\n",
    "\n",
    "In neural networks, all of these approaches still rely on gradient descent. So, no, the global optima is not guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 8 - Model\n",
    "\n",
    "**Suppose you have the following training data set and suppose you expect your training data set to be representative of your test data. Of all of the methods we can covered in the course, which would you use to classify this data into black x vs. red o. Why? Justify your answer.**\n",
    "\n",
    "<div><img src=\"model.png\", width=\"300\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first observation from this data set is that classes are not linearly separable. Therefore, models such as LDA and Perceptron will not be successful learning these classes.\n",
    "\n",
    "An MLP can be used for this data. An architecture of 2-19-1 can work for this data set. We can considered about 19 units in the hidden layer as they represent the 19 linear boudaries that need to be places diagonally to separate regions of class 'x' and class 'o'.\n",
    "\n",
    "The k-NN model is also able to learn such spaces but it is highly proned to overfitting. Furthermore, empty regions (e.g. around point ~[9,3]) without training samples representations may cause the test to perform poorly.\n",
    "\n",
    "A type of model that is \"designed\" for this type of problems is SVM. SVM equipped with an RBF kernel is able to project this data into a higher dimensional space where the classes are linearly separable and easy to separate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 9 - Neural Network\n",
    "\n",
    "**Consider the following two-dimensional data set and desired values for a two-class classification problem:**\n",
    "\n",
    "|  $x_1$  |  $x_2$  |    d    |\n",
    "|---------|---------|---------|\n",
    "|    0    |    0    |    0    |\n",
    "|  -0.01  |   0.01  |    0    |\n",
    "|   0.05  |   0.05  |    0    |\n",
    "|    0    |    1    |    1    |\n",
    "|  -0.01  |   1.05  |    1    |\n",
    "|   0.01  |   0.99  |    1    |\n",
    "|    1    |    0    |    1    |\n",
    "|   1.05  |  -0.06  |    1    |\n",
    "|   1.01  |   0.04  |    1    |\n",
    "|    1    |    1    |    0    |\n",
    "|   1.01  |   1.02  |    0    |\n",
    "|   0.98  |   0.99  |    0    |\n",
    "\n",
    "**Define a neural network structure and associated parameter values that can solve this classification problem (with zero error on this data set).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set can be seen in the figure below:\n",
    "\n",
    "<div><img src=\"problem9_solution.png\", width=\"500\"><!div>\n",
    "    \n",
    "This data set is a noisy XOR problem. Therefore, we will need one hidden layer MLP to solve this problem. I chose the two linear boundaries depicted in the figure above but other choices are also possible.\n",
    "\n",
    "The equations for these boundaries are:\n",
    "\n",
    "$$L_1: x_2 = x_1 + 0.5$$\n",
    "\n",
    "$$L_2: x_2 = x_1 - 0.5$$\n",
    "    \n",
    "The final network architecture with all parameters is as follows:\n",
    "    \n",
    "<div><img src=\"problem9_net.png\", width=\"700\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 10 - Regularization in Neural Networks\n",
    "\n",
    "**Assume you have a network with a single output neuron. Assume you would like to use gradient descent to minimize the objective function and you want to attempt to prevent overtraining through weight decay regularization (L2-norm). Derive the gradient descent update equation for a weight at the output layer neuron by minimizing the following objective function:**\n",
    "\n",
    "$$J = \\frac{1}{2} \\sum_{i=1}^N e_i^2 + \\lambda R(w)$$\n",
    "\n",
    "**where $e_i$ is the error given the current network parameters and the desired output, $d_i$, for data point $x_i$, and $R(w)$ is the weight decay term. Keep your result general and applicable for any activation function, $\\phi(\\bullet)$. Clearly define any notation you use.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight decay regularization term corresponds to the L2-norm penalty.\n",
    "\n",
    "The cost function with the L2 penalty on the weights at the output layer, $w_j$, is given as follows:\n",
    "\n",
    "$$J = J_e + \\lambda J_r = \\frac{1}{2}\\sum_{i=1}^N e_i^2 + \\lambda \\sum_{j=1}^M$$\n",
    "\n",
    "The gradient descrent update for the weights at the output layer is given as follows:\n",
    "\n",
    "\\begin{align}\n",
    "w_j^{(t+1)} &= w_j^{(t)} - \\eta \\frac{\\partial J}{\\partial w_j} \\\\\n",
    "w_j^{(t+1)} &= w_j^{(t)} - \\eta \\left(\\frac{\\partial J_e}{\\partial w_j} + \\lambda\\frac{\\partial J_r}{\\partial w_j} \\right)\\\\\n",
    "w_j^{(t+1)} &= w_j^{(t)} - \\eta \\left(-e_i\\phi'(v_j)y_j + \\lambda 2w_j^{(t)} \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 11 - Unknown Class\n",
    "\n",
    "**Suppose you trained an MLP using standard backpropagation on a training set that contained two classes. Suppose, you designed your MLP to have a single output neuron and you trained the MLP to map the input data to the class labels -1 or 1. During test, data from a third class was put through your trained network for classification. Is it possible for the MLP you trained to be able to identify and correctly classify data from the third class? Why or Why not? What strategies can you employ during training to guard against this situation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. The third class is not learnt to generalize the network, it will not give a result that discriminates it from class 1 and class 2.\n",
    "\n",
    "However, we can threshold the output probability to make the network more certain about prediction. In this way, we will be able to consider uncertain samples as an unknown class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 12 - Deep Learning\n",
    "\n",
    "**What is the difference between Deep Learning and Machine Learning? When would you use or the other?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning refers to the process of having a machine to learn patterns in data. Machine Learning is typically accompanied by the design of hand-crafted features.\n",
    "\n",
    "Deep Learning is a subgroup of Machine Learning. Deep Learning uses deep neural network architectures with self-generating features. Therefore, Deep Learning completely bypasses the need to manual feature extraction, rather it learns the necessary features to discriminante data needing only data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 13 - Convolutional Neural Networks\n",
    "\n",
    "1. **How are convolutional neural networks different from MLPs?**\n",
    "\n",
    "2. **Why are they a class of deep learning models?**\n",
    "\n",
    "3. **Discuss advantages and disavantages of training a CNN for data classification.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CNNs are weight-shared networks. The parameter space of a CNN is much smaller than a MLP. In addition, CNNs have convolutional layers that employ convolution operations to extract particular features. An MLP could also learn a convolutional operation but without any weight-sharing restrictions, it will be very unlikely.\n",
    "\n",
    "2. CNNs are a class of deep learning models as they carry self-feature generation. CNNs are also typical deep architectures.\n",
    "\n",
    "3. CNN has a clear advantage over MLP simply for the fact that features can be learned without the need of user input. CNN parameter space is greatly reduced which helps prevent overfitting. CNNs need a great amound of data is needed to learn *important* features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 14 - Pipeline\n",
    "\n",
    "**Suppose you are interested in performing classification of a data set with $M$ classes.**\n",
    "\n",
    "1. **Describe the standard pipeline approach of a Machine Learning algorithm. List all steps and its function.**\n",
    "\n",
    "2. **Describe the standard pipeline approach of a Deep Learning algorithm. List all steps and its function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Step 1**: Data Acquisition. **Step 2**: Feature Generation feature extraction and/or feature selection. includes hand-crafting of features that will be used to discriminate between classes. **Step 3**: Pre-processing. Includes data scaling and data transformation. **Step 4**: Model Selection. Involves selecting a model and its hyper-parameters. **Step 5**: Hyper-parameter tuning. Involves training the model and fine-tuning its hyper-parameters for a given data set. **Step 6**: Model Evaluation.\n",
    "\n",
    "2. **Step 1**: Data Acquisition. **Step 2**: Pre-processing. Includes data scaling and data transformation. **Step 3**: Model Selection. Involves selecting a model and its hyper-parameters. **Step 4**: Hyper-parameter tuning. Involves training the model and fine-tuning its hyper-parameters for a given data set. **Step 5**: Model Evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 15 - Avoiding Overfitting\n",
    "\n",
    "**What are some strategies that we can use to avoid overfitting while training a neural network. Provide a brief description for all items listed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training a neural network, there are quite a few strategies that can be used to avoid overfitting. They include:\n",
    "\n",
    "* Adding more data. \n",
    "* Cross-validation. Make use of a validation set during training to check the network's generalization ability.\n",
    "* Regularization. We can add a regularization to penalize weights from being too large.\n",
    "* Early Stopping. Training can be stopped when a certian number of epochs did not improve the overall error/accurac or the model.\n",
    "* Batch normalization. At each layer, we can perform normalize the output values before feeding them to the next layer. This will place the output values within a fixed interval.\n",
    "* Boosting. Assign weights to *difficult* samples.\n",
    "* Bagging. Train the same model using multiple Bootstrap samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 16 - Accerated Learning & Adaptive Learning\n",
    "\n",
    "**What are some strategies that we can use to accelerate the learning process in a neural network? Provide a brief description for all items listed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning process with gradient descent can be accelerate using a momentum term. Typical learning strategies are Stochastic Gradient Descent (SGD) with momentum or SGD with Nestervo's momentum.\n",
    "\n",
    "We can also adaptively change the learning rate value. In addition to adding a momentum term to speed up learning, the ADAM algorithm also adaptively changes the value of the learning rate as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 17 - Take your pick\n",
    "\n",
    "**Suppose a group of friends are going to train a neural network for the first time to perform classification, and they ask you (the ML expert) which network architecture they should use in order to avoid overfitting. Would you recommend them to use a single hidden layer network with a large number of units in the hidden layer, or would you recommend them to use a network with several layers (e.g., ten layers)? Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deeper network is more prone to overfitting as adding layers create a complex feature representation.\n",
    "\n",
    "In order to avoid overfitting, a shallow network is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
