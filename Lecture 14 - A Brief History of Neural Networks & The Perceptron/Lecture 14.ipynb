{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 14 - Introduction to Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put simply, Artificial Neural Networks (ANNs) were *born* with the goal of extracting *knowledge* from data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where does knowledge come from?\n",
    "\n",
    "But before we introduce and motivate ANNs, I believe it is important to point out that there are many different ways of extracting *knowledge* from data. \n",
    "\n",
    "* Where does knowledge come from?\n",
    "    * Evolution (which is encoded in our DNA)\n",
    "    * Experience (which is encoded in our brains)\n",
    "    * Culture\n",
    "\n",
    "* There is also another emergent source of knowledge: computers! As Yann LeCun, co-recipient of Turing Award, and Chief AI Scientist at Facebook said: *Most of the knowledge in the world in the future is going to be extracted by machines and will reside in machines.\n",
    "\n",
    "And so, how do computers discover new knowledge? Pedro Domingos, the author of the book *Master Algorithm*, posed it in terms of the *Five Tribes of Machine Learning*:\n",
    "\n",
    "1. Fill in gaps in existent knowledge - The Symbolists\n",
    "\n",
    "2. Emulate the brain - The Connectionists\n",
    "\n",
    "3. Simulate evolution - The Evolutionaries\n",
    "\n",
    "4. Systematically reduce uncertainty - The Bayesians\n",
    "\n",
    "5. Notice similarities between the old and new - The Analogizers\n",
    "\n",
    "All of theses \"tribes\" have its own general purpose *learner* that in principle can be used to learn anything. In fact, in all of them there are mathematical proves that essentially say if you give enough that it can learn anything. \n",
    "\n",
    "For The Symbolistists it is inverse deduction, for The Connectionists it's backpropagation, for The Evolutionaries it's genetic programming, for The Bayesians it's probabilistic inference using Bayesian statistics and for The Analogizers it's kernel machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Symbolists\n",
    "\n",
    "The general purpose learner of *The Symbolists* is inverse deduction.\n",
    "\n",
    "* Inverse deduction is induction.\n",
    "\n",
    "* Deduction is making a specific conclusion based on the general, such as if A then B.\n",
    "\n",
    "* Induction is where you prove an individual instance is true, and then by using reasonable operators, show that it would be true of all other instances as well, thus arriving at a general conclusion from the specific.\n",
    "\n",
    "Symbolic artificial intelligence is the collective name for all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search. Symbolic AI was the dominant paradigm of AI research from the middle fifties until the late 1980s.\n",
    "\n",
    "* *Expert systems* that are popular forms of symbolic AI use a network of production rules that connect symbols in a relationship similar to an If-Then statement.\n",
    "\n",
    "![Robot Eve](https://www.wired.com/images_blogs/wiredscience/2011/04/robot-scientist-adam-ross-king-aberystwyth.jpg)\n",
    "[*Robot “Scientist” Helps Discover New Ingredient for Antimalarial Drug*](https://futurism.com/robot-scientist-helps-discover-new-ingredient-antimalarial-drug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Evolutionaries\n",
    "\n",
    "The general purpose learner of *The Evolutionaries* is (you guessed it) evolution.\n",
    "\n",
    "* Turning Darwin's theory into an algorithm\n",
    "\n",
    "![Genetic Algorithm](https://www.raymanning.com/img/genflow.jpg)\n",
    "\n",
    "You can use Genetic Algorithms (GAs) on computer programs in order to learn the next generation of programs that best perform/fit a certain goal. You can take a step forward and actually implement GAs in robots.\n",
    "\n",
    "* Here's an interesting video: [Hod Lipson - Building \"Self-aware\" robots](https://www.ted.com/talks/hod_lipson_building_self_aware_robots?language=en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayesians\n",
    "\n",
    "The general purpose learner of *The Bayesians* is probabilistic inference using Bayesian statistics.\n",
    "\n",
    "![Bayes' Theorem](https://miro.medium.com/max/500/1*2Ixe8hsTASXjMXt9TySHGA.png)\n",
    "\n",
    "Example algorithms:\n",
    "* Maximum A Posteriori (MAP) likelihood\n",
    "* Maximum Likelihood Estimator (MLE)\n",
    "* Bayesian Networks\n",
    "\n",
    "Applications:\n",
    "\n",
    "* Risk assessment for self-driving cars\n",
    "* Anti-spam filters\n",
    "* Novelty detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Analogizers\n",
    "\n",
    "The general purpose learner of *The Analogizers* are kernel machines.\n",
    "\n",
    "![Kernel Machines](https://qph.fs.quoracdn.net/main-qimg-cd6cde306c8273b2af183f57f25c259d)\n",
    "\n",
    "Example algorithms:\n",
    "* SVMs\n",
    "* k-Nearest Neighbors\n",
    "\n",
    "Some applications:\n",
    "* Recommender systems (Netflix, Amazon)\n",
    "* Classification of satellite data like SAR data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Connectionists\n",
    "\n",
    "The general purpose learner of *The Connectionists* is backpropagation.\n",
    "\n",
    "The Connectionists take inspiration in the human brain and build models that try to recreate it. And so, the most popular model is the Artifical Neural Network (ANN) model and all its variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons and Neural Networks in the Human Brain\n",
    "\n",
    "The brain is a highly complex, nonlinear and parallel information-processing system composed of *neurons*.\n",
    "\n",
    "At birth, a brain has the ability to build up its own rules through experience. The most dramatic development of the human brain takes place during the first two years from birth.\n",
    "\n",
    "The neuron in the human brain is typically five to six orders of magnitude slower than silicon logic gates ( $10^{-3}$s/millisecond vs. $10^{-9}$s/nanosecond) but makes it up by having an enormous number of massively interconnected neurons\n",
    "\n",
    "* Neurons in the human brain:\n",
    "\n",
    "<div><img src=\"figures/neuron_structure.jpg\", width=\"700\"><!div>\n",
    "\n",
    "* **Synapse:** elementary structural and functional units that control *interaction* between neurons. *Chemical synapse*, most common kind of synapse, converts a presynaptic electrical signal into a chemical signal and then back into a postsynaptic electrical signal. Traditional descriptions of assume that a synapse can either impose *excitation* or *inhibition* (but not both) on the receptive neuron.\n",
    "\n",
    "<div><img src=\"figures/synapse.png\", width=\"500\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief History of Artificial Neural Networks\n",
    "\n",
    "<div><img src=\"figures/NN_timeline.png\", width=\"1000\"><!div>\n",
    "\n",
    "* 1943 McCulloch and Pitts. McCulloch was a psychatrist and neuroanatomist. Pitts was a mathematician. They published a widely read article that introduced the idea of neural networks as computing machines. Their goal was to develop a model/understand how neurons in the brain might work. They showed a range of arithmetic and logical functions their neuron could compute.  \n",
    "\n",
    "* 1949 Hebb wrote \"The Organization of Behavior\" which postulated (among many other things): \"When an axon of cell A is near enough to excite cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased\"  or \"when neurons fire together they wire together\". This has come to be known as *Hebbian learning*. \n",
    "\n",
    "* 1954 Minsky wrote a \"neural network\" doctoral thesis at Princeton. \n",
    "\n",
    "* 1958 Rosenblatt introduced his work in the perceptron and he came up with the perceptron convergence theorem. First model for learning with a teacher (i.e., \"supervised learning\").\n",
    "\n",
    "* 1969 Minsky and Papert demonstrated the limits of the perceptron. They introduce multi-layer perceptrons but the published limits had the biggest influence - and interest dropped away. The AI winter began. \n",
    "\n",
    "* 1974 Werbos' Ph.D. thesis at Harvard developed back-propagation.\n",
    "\n",
    "* 1986, the book \"Parallel Distributed Processing: Explorations in the Microstructures of Cognition\" was published and it covered back-propagation. This made Neural Networks popular again.\n",
    "\n",
    "* In 1989, Yann LeCun used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers.\n",
    "\n",
    "* Early 90's, Support Vector Machines (SVMs) overtook ANNs in popularity due to a number of challenges/downsides to ANNs in comparison to SVMs.  This included that SVMs were less likely to overtrain and easier to get good results on. Also, ANNs were very slow to train and had issues when they became \"deep\".\n",
    "\n",
    "* 2012 ImageNet challenge won by Hinton's team using a deep CNN (based on top 5 error rate, given an image, the model does not output the correct label within its top 5 predictions). They had an error rate of 15.4\\% (which was way better than 2nd place at 26.6\\%). This started the current DL/ANN resurgance. Now it's huge.\n",
    "\n",
    "* *What do you think will happen next?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rosenblatt's Perceptron\n",
    "\n",
    "![Brainy Computer 1955](http://blog.modernmechanix.com/mags/PopularMechanics/5-1955/brainy_computer.jpg)\n",
    "<center> Popular Mechanics 5-1955 </center>\n",
    "\n",
    "![The Perceptron](http://www.rutherfordjournal.org/images/TAHC_perceptron.jpg)\n",
    "\n",
    "![Character Recognition](https://news.cornell.edu/sites/default/files/styles/full_size/public/2019-09/0925_rosenblatt5.jpg?itok=YuOn88nZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rosenblatt's Perceptron\n",
    "\n",
    "In an Artificial Neural Network (ANN or NN), we attempt to model the human neural network and neurons using programming constructs.\n",
    "\n",
    "A basic model for a neuron consists of the following: \n",
    "\n",
    "* A set of *synapses* each of which is characterized by a *weight* (which includes a *bias*).\n",
    "\n",
    "* An *adder*.\n",
    "\n",
    "* An *activation function* (e.g. heaviside function, piece-wise linear function, sigmoid function, ReLu, Leaky ReLu etc.)\n",
    "\n",
    "<div><img src=\"figures/modelneuron.png\", width=\"500\"><!div>\n",
    "\n",
    "<div><img src=\"figures/act1.png\", width=\"300\"><!div>\n",
    "\n",
    "We can write this mathematically as: \n",
    "\n",
    "$$y_k = \\varphi\\left( \\sum_{j=1}^m w_{kj}x_j + b_k\\right)$$\n",
    "\n",
    "* *What does this look like graphically?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Activation Functions\n",
    "\n",
    "In artificial neural networks (ANN), the *activation function* of a neuron defines the output of that neuron given an input or set of inputs.\n",
    "\n",
    "There are many activation functions, the most common are:\n",
    "\n",
    "1. **Heaviside step function:**\n",
    "$$\\phi(x) = \\begin{cases}1, & x >0 \\\\ 0, & x\\leq 0\\end{cases}$$\n",
    "\n",
    "2. **Sigmoid function:**\n",
    "$$\\phi(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "3. **Hyperbolic tangent function (tanh):**\n",
    "$$\\phi(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "4. **Rectified Linear (ReLu) function:**\n",
    "$$\\phi(x) = \\begin{cases} x, & x\\geq 0 \\\\ 0, & x < 0\\end{cases}$$\n",
    "\n",
    "5. **Leaky ReLu function:**\n",
    "$$\\phi(x) = \\begin{cases} x, & x\\geq 0 \\\\ 0.01x, & x < 0\\end{cases}$$\n",
    "\n",
    "There are many many other functions ([Wikipedia link](https://en.wikipedia.org/wiki/Activation_function))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Learning Algorithm\n",
    "\n",
    "Suppose we have a \"neural network\" made of only one neuron - i.e., **Rosenblatt's perceptron** and we would like to train it to distinguish between two classes; where the activation function is:\n",
    "\n",
    "$$y_i = \\phi(x_i) = \\begin{cases} 1 & \\text{if }w_{ki}\\mathbf{x}_i+b_k>0\\\\ 0 & \\text{otherwise}\\end{cases}$$\n",
    "\n",
    "For Rosenblatt's perceptron to be effective, the classes must be linearly separable. Mathematically, the classes must satisfy:\n",
    "\n",
    "$$\\exists \\mathbf{w} \\mid \\mathbf{w}^T\\mathbf{x} +b > 0, \\quad \\forall \\mathbf{x} \\in C_1;  \\quad \\mathbf{w}^T\\mathbf{x} +b\\leq 0, \\quad \\forall \\mathbf{x} \\in C_2$$\n",
    "\n",
    "* The Perceptron Learning Algorithm will converge to the correct solution if the classes are linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an alternative error function known as the *perceptron criterion*. To derive this, we note that we are seeking a weight vector $\\mathbf{w}$ such that patterns $x_i$ in class $C_1$ will have $\\mathbf{w}^Tx_i + b > 0$, whereas the patterns $x_i$ in class $C_2$ have $\\mathbf{w}^Tx_i + b < 0$. Using the $t\\in \\{-1,1\\}$ target coding scheme it follows that we would like all patterns to satisfy\n",
    "\n",
    "$$(\\mathbf{w}^Tx_i + b)t_i > 0$$\n",
    "\n",
    "* The perceptron criterion associates zero error with any pattern that is correctly classified, whereas for a misclassified pattern $x_i$ it tries to minimize the quantity $-(\\mathbf{w}^Tx_i + b)t_i$.\n",
    "\n",
    "* The perceptron criterion is therefore given by:\n",
    "\n",
    "$$E_p(\\mathbf{w},b) = - \\sum_{n\\in\\mathcal{M}} (\\mathbf{w}^T \\mathbf{x}_n + b) t_n$$\n",
    "\n",
    "where $\\mathcal{M}$ denotes the set of all misclassified patterns.\n",
    "\n",
    "* We now apply the *stochastic gradient descent* algorithm to this error function. The change in the weight vector $\\mathbf{w}$ is then given by:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{w}^{(t+1)} &\\leftarrow & \\mathbf{w}^{(t)} - \\eta \\frac{\\partial E_p(\\mathbf{w},b)}{\\partial \\mathbf{w}} = \\mathbf{w}^{(t)} + \\eta \\mathbf{x}_n t_n\\\\\n",
    "b^{(t+1)} &\\leftarrow & b^{(t)} - \\eta\\frac{\\partial E_p(\\mathbf{w},b)}{\\partial b} = b^{(t)} + \\eta t_n\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\eta$ is the **learning rate** parameter and $t$ is an integer that indexes the iteration steps of the algorithm. \n",
    "\n",
    "* Note that, as the weight vector evolves during training, the set of patterns that are misclassified will change.\n",
    "\n",
    "<div><img src=\"figures/PerceptronLearning.png\", width=\"600\"><!div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def generateMVNRandData(Npts, mu, sigma):\n",
    "    data  = np.random.multivariate_normal(mu, sigma*np.eye(len(mu)), Npts)\n",
    "    return data\n",
    "\n",
    "def plotLine(weights, range):\n",
    "    x = np.array(range)\n",
    "    y = -(weights[0]/weights[1])-(weights[2]/weights[1])*x\n",
    "    plt.plot(y,x)\n",
    "\n",
    "def perceptronLearningAlg(data,labels,eta,nIterations):\n",
    "    nPts = data.shape[0]\n",
    "    weights = np.random.rand(data.shape[1])\n",
    "    print('Initial weights:', weights)\n",
    "    \n",
    "    error = 1\n",
    "    iter = 0\n",
    "    \n",
    "    # TO BE COMPLETED IN CLASS\n",
    "    \n",
    "#                 plt.scatter(data[:,1],data[:,2], c=labels, linewidth=0)\n",
    "#                 plotLine(weights, [-2,2]);\n",
    "#                 plt.pause(2)\n",
    "                \n",
    "    print('Final Iteration: ', iter,'; Final Error: ', error)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Npts  = 100\n",
    "mu1   = [2,2]\n",
    "mu2   = [0,0]\n",
    "var   = .1\n",
    "eta   = 0.3\n",
    "nIterations = 10;\n",
    "\n",
    "data1 = np.array(generateMVNRandData(Npts, mu1, var))\n",
    "data1 = np.hstack((np.ones((Npts,1)),data1))\n",
    "\n",
    "data2 = np.array(generateMVNRandData(Npts, mu2, var))\n",
    "data2 = np.hstack((np.ones((Npts,1)),data2))\n",
    "\n",
    "data  = np.vstack(( data1, data2))\n",
    "labels= np.hstack((np.ones(Npts), -np.ones(Npts)))\n",
    "\n",
    "plt.scatter(data[:,1],data[:,2], c=labels, linewidth=0)\n",
    "\n",
    "perceptronLearningAlg(data,labels,eta,nIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food for Thought\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "1. Consider a neuron with two inputs and one output and a step function. If two weights are $w_1=1$ and $w_2 =1$, and the bias is $b=-1.5$, then what is the output for inputs $(0,0)$, $(1,0)$, $(0,1)$, and $(1,1)$?\n",
    "\n",
    "2. How does the learning behavior change with changes in $\\eta$? as $\\eta$ increases? as $\\eta$ decreases?\n",
    "\n",
    "3. How would you generate overlapping classes using the provided code? Explain your answer. (Only change parameters. You do not need to change code.)\n",
    "\n",
    "4. What happens to the learning behavior when you have overlapping classes?\n",
    "\n",
    "5. The implementation provided uses $\\{-1,1\\}$ labels. Suppose we want to use labels $\\{0,1\\}$. How can we formulate the Perceptron Learning? How does the code need to change to account for this difference (i.e., suppose you want to use $\\{0,1\\}$ labels. What would you need to change in the code?) Why?\n",
    "\n",
    "6. In the provided code, there is not a separate line for learning the bias $b$ as in the pseudo-code above. How is it being estimated and represented it in the code? (... the code *is* still learning the bias value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Multilayer Perceptron - Exercise\n",
    "\n",
    "Suppose you had the following neural network:\n",
    "\n",
    "<div><img src=\"figures/MLP.png\", width=\"300\"><!div>\n",
    "\n",
    "with a heaviside activation function:\n",
    "\n",
    "$$\\phi(x) = \\begin{cases} 1 & x > 0 \\\\ -1 & x \\leq 0 \\end{cases}$$\n",
    "\n",
    "1. What is the expression of the output value $y$ in terms of the input values?\n",
    "\n",
    "2. What is the output with the following input values?\n",
    "    * $[0,0]$\n",
    "    * $[-2, -2.5]$\n",
    "    * $[-5, 5]$\n",
    "    * $[10, 3]$\n",
    "\n",
    "3. What does the decision surface of this network look like graphically? Draw it out by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
