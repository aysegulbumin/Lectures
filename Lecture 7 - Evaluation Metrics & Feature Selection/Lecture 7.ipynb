{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7 - Evaluation Metrics & Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "* So far, we have focused on regression. We will begin to discuss classification.\n",
    "\n",
    "* The goal in **classification** is to take an input vector, **X**, and assign it to one of $K$ discrete classes.  \n",
    "\n",
    "* Usually this is done by learning **decision boundaries** or **decision surfaces** that divide up the feature space and assign a class to each partition of the feature space created by the decision boundary. \n",
    "\n",
    "* Data whose classes can be separated perfectly with a linear decision surface (i.e., a line in 2-D, a plane in 3-D, etc) are called **linearly separable**. These are the easiest classification problems to address. \n",
    "\n",
    "    * There are *many* classifiers in the machine learning literature. We will cover a few in this class.\n",
    "\n",
    "* You can **encode class labels** in a number of ways: binary vectors, class number, etc. Different encodings make sense in different situations. But you want to select carefully - depending on the algorithm you use, it can make a difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Workflow of a Supervised Learning System\n",
    "\n",
    "1. **Data Acquisition**\n",
    "\n",
    "One of the most (if not the one) crucial steps in building a supervised learning system. The collected data will have a tremendous impact on the subsequent steps of the machine learning system including: feature extraction, model selection and model validation.\n",
    "\n",
    "2. **Feature Extraction**\n",
    "\n",
    "Sometimes the data you are working with was already collected and features were extracted, so you will have little control of the first two steps, other than some feature engineering you may wish to apply.\n",
    "When you do have control of your data, feature extraction will also be an important step. We will discuss more details on this broad topic, specifically for image data.\n",
    "\n",
    "3. **Pre-processing**\n",
    "\n",
    "In this stage you will prepare the data to be *fed* into the system. Data pre-processing include **normalization**. This can be a big difference in the results. Always normalize the data.\n",
    "\n",
    "4. **Model Selection**\n",
    "\n",
    "Selecting the model can be influenced by any information you may have about the data.\n",
    "\n",
    "5. **Hyperparameter Tuning**\n",
    "\n",
    "In this step you will be mostly trying to find the set of parameters that fit the data the best but avoid overfitting. This includes implementing strategies such as: regularization, k-fold cross-validation and plotting learning curves.\n",
    "\n",
    "There are useful modules from the ```scikit-learn``` library that you should take some time reading and learning about: from the module ```preprocessing```, read about function ```StandardScaler```; from the module ```cross_validation```, read about function ```cross_val_score```; from module ```learning_curve```, read about function ```learning_curve```; from module ```grid_search```, read about function ```GridSearchCV```. \n",
    "\n",
    "6. **Model Evaluation**\n",
    "\n",
    "The last step of a supervised learning system is to report on the model's performance. This may include any statistical measures such as: mean average error, Quantile-Quantile (Q-Q) plot and hypothesis tests. If you want to answer the question \"Is A a more accurate algorithm than B?\", this becomes the hypothesis \"Can we say that the average error of learners trained by A is significantly lower than the average error of learners trained by B?\". So we can design statistical tests and measures to evaluate the models.\n",
    "\n",
    "But you can also look at different performance metrics. This next section will discuss different performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation Metrics\n",
    "\n",
    "A key step in supervised learning system workflow is determining an *appropriate* performance evaluation metric.\n",
    "\n",
    "**Evaluation metrics** help us to estimate how well our model is trained and it is important to pick a metric that matches our overall goal for the system.\n",
    "\n",
    "Some common evaluation metrics include precision, recall, receiver operating curves, and confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Accuracy and Error\n",
    "\n",
    "Classification accuracy and e the number of correct predictions made as a ratio of all predictions made.\n",
    "\n",
    "* **Classification accuracy** is defined as the number of correctly classified samples divided by all samples:\n",
    "\n",
    "$$\\text{accuracy} = \\frac{N_{\\text{corr}}}{N}$$\n",
    "\n",
    "where $N_{\\text{corr}}$ is the number of correct classified samples and $N$ is the total number of samples.\n",
    "\n",
    "* **Classification error** is defined as the number of incorrectly classified samples divided by all samples:\n",
    "\n",
    "$$\\text{error} = \\frac{N_{\\text{miss}}}{N}$$\n",
    "\n",
    "where $N_{\\text{miss}}$ is the number of misclassified samples and $N$ is the total number of samples.\n",
    "\n",
    "* Classification accuracy is the most common evaluation metric for classification problems, it is also the most misused. It is really only suitable when there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case.\n",
    "\n",
    "### Example 1: Fish Dataset\n",
    "Suppose there is a 3-class classification problem, in which we would like to classify each training sample (a fish) to one of the three classes (A = salmon or B = sea bass or C = cod).\n",
    "\n",
    "Let's assume there are 150 samples, including 30 salmon, 40 sea bass and 80 cod. Suppose our model misclassifies 4 salmon, 2 sea bass and 5 cod.\n",
    "\n",
    "* The classification accuracy (ACC) of our binary classification model is calculated as:\n",
    "\n",
    "$$\\text{ACC} = \\frac{26 + 38 + 75}{30 + 40 + 80} = \\frac{139}{150} \\approx 92.7 \\%$$\n",
    "\n",
    "* The prediction error is calculated as:\n",
    "\n",
    "$$\\text{error} = \\frac{4 + 2 + 5}{30+40+80} = \\frac{11}{150} \\approx 7.3 \\%$$\n",
    "\n",
    "* The classification accuracy doesn't really give an insight on which class is being misclassified the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix summarizes the classification accuracy across several classes. It shows the ways in which the classification model is confused when it makes predictions, allowing visualization of the performance of our algorithm. \n",
    "\n",
    "Generally, each row represents the instances of a actual class while each column represents the instances in an predicted class.\n",
    "\n",
    "If the classifier is trained to distinguish between salmon, sea bass and cod. We can summarize the prediction result in the confusion matrix as follows:\n",
    "\n",
    "|actual/predict|    salmon    |    sea bass  |      cod     |\n",
    "|--------------|--------------|--------------|--------------|\n",
    "|    salmon    |      26      |       2      |       2      |\n",
    "|    sea bass  |       2      |       38     |       3      |\n",
    "|      cod     |       2      |       3      |       75     |\n",
    "\n",
    "\n",
    "In this confusion matrix, of the 30 salmons (row 1), the classifier predicted that 26 are labeled salmon correctly, 2 are wrongly labeled as sea bass, and another 2 are wrongly labeled as cod. \n",
    "\n",
    "All correct predictions are located in the diagonal of the table. So it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall & Fall-Out\n",
    "\n",
    "We are often looking to discriminate between observations with a specific binary outcome, for example, event or no event. In our example, the fish company would like to produce salmon can but the harvest contains all three species. In this way,\n",
    "we can assign the event (salmon) as \"positive\" and no-event (not salmon) as \"negative\".\n",
    "\n",
    "The confusion matrix for this two-class classification problem is:\n",
    "\n",
    "|actual/predicted|    salmon    |  non-salmon  |\n",
    "|----------------|--------------|--------------|\n",
    "|     salmon     |      26      |       4      |\n",
    "|   non-salmon   |       7      |      113     |\n",
    "\n",
    "* **True positive (TP):** correctly predicting positive events\n",
    "* **False positive (FP):** incorrectly calling positive to a negative event\n",
    "* **True negative (TN):** correctly predicting negative events\n",
    "* **False negative (FN):** incorrectly labeling negative to a positive event\n",
    "\n",
    "*In this salmon/non-salmon classification problem, what are the TP, FP, TN, FN values?*\n",
    "\n",
    "|actual/predicted|   Positive   |   Negative   |\n",
    "|----------------|--------------|--------------|\n",
    "|    Positive    |      TP      |      FN      |\n",
    "|    Negative    |      FP      |      TN      |\n",
    "\n",
    "* **Precision**, also called Positive Predictive Value (PPV), is the performance of detection\n",
    "\n",
    "$$\\text{Precision} = \\text{PPV} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "* **Recall**, also called True Positive Rate (TPR) or Sensitivity, is the probability of detection\n",
    "\n",
    "$$\\text{Recall} = \\text{TPR} = \\text{Sensitivity} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "* **Fall-out**, also called False Positive Rate (FPR), is the probability of false alarm\n",
    "\n",
    "$$\\text{Fall-out} = \\text{FPR} = \\frac{FP}{FP + TN}$$\n",
    "\n",
    "* **Specificity**, also called True Negative Rate (TNR), is the probability of negative events detection\n",
    "\n",
    "$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "<!--* **F1-score**, also called F-score or F-measure, is a measure of a model's accuracy. It considers both the precision and the recall-->\n",
    "\n",
    "<!--$$\\text{F1-score} = 2\\times\\frac{\\text{Precision}\\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$-->\n",
    "\n",
    "* Learn about many other measures on the [Wikipedia page](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) and [Scikit-Learn Metrics Module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves\n",
    "\n",
    "**Receiver Operating Characteristic (ROC) curve** is the plot between the true positive rate (TPR) and the false positive rate (FPR), where the TPR is defined as the y-axis and FPR is defined as the x-axis.\n",
    "\n",
    "* ROC curves were first developed for RADAR systems, hence the name.\n",
    "\n",
    "* Given a binary classifier and its threshold, the (x,y) coordinates of ROC space can be calculated from all the prediction result. You trace out a ROC curve by varying the threshold to get all of the points on the ROC.\n",
    "\n",
    "* The diagonal between (0,0) and (1,1) separates the ROC space into two areas, which are left up area and right bottom area. The points above the diagonal represent good classification (better than random guess) which below the diagonal represent bad classification (worse than random guess).\n",
    "\n",
    "* *What is the perfect prediction point in a ROC curve?*\n",
    "\n",
    "\n",
    "### Area Under the Curve (AUC)\n",
    "\n",
    "**Area Under Curve (AUC)** is a common measure of how good a test is. It is simply the area under the ROC curve. Random guessing can achieve the diagonal line, so the minimum AUC is 1/2. The maximum AUC is 1, which is achieved by a test that is always right; the ROC curve is along the left and top axes.\n",
    "\n",
    "### Example\n",
    "\n",
    "1. Suppose you have a target detection task that you would like to evaluate using ROC curve analysis. You emplaced 10 targets and collected aerial hyperspectral imagery over 10 $km^2$. Then, suppose you ran a set of alarm generation and target detection algorithms over the collecte data. Your algorithms produced the following list of alarm confidence values. You have already matched each of these alarms to a location on the ground and compared them with you ground truth. True targets, based on your ground truth, are marked with a \"T\" in the second column. Draw the associated ROC cure for these results.\n",
    "\n",
    "|  0.91  |  0.90  |  0.80  |  0.79  |  0.77  |  0.75  |  0.50  |  0.40  |  0.39  |  0.38  |  0.37  |  0.25  |  0.10  |  0.09  |  0.01  |\n",
    "|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n",
    "|   T    |   T    |        |   T    |        |        |        |   T    |        |        |        |        |        |   T    |        |\n",
    "\n",
    "\n",
    "2. Suppose you were segmenting a data set into three classes (e.g., vegetation, man-made materials, sand) and wanted to evaluate your results. Would sing a ROC curve be an appropriate method for evaluation? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check the list of performance evaluation metrics in the scikit-learn module called \"metrics\". Find it [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Suppose that you are building the experimental design for your machine learning algorithm from a provided data set. Your first setp should include feature selection. \n",
    "\n",
    "In this section, we will discuss a few techniques to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Wine Dataset\n",
    "\n",
    "Let's work through with the Wine dataset, an open-source dataset that is available from the [UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/Wine); it consists of 178 wine samples with 13 features describing their different chemical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine.columns = ['Class label', 'Alcohol',\n",
    "                   'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium',\n",
    "                   'Total phenols', 'Flavanoids',\n",
    "                   'Nonflavanoid phenols',\n",
    "                   'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue',\n",
    "                   'OD280/OD315 of diluted wines',\n",
    "                   'Proline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples belong to one of three different classes, 1, 2, and 3, which refer to the three different types of grapes that have been grown in different regions in Italy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning a dataset in training and test sets\n",
    "\n",
    "A convenient way to randomly partition this dataset into a separate test and training dataset is to use the ```train_test_split``` function from scikit-learn's model_selection submodule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing features onto the same scale\n",
    "\n",
    "**Feature scaling** is a crucial step in our pre-processing pipeline that can easily be forgotten. Decision trees and random forests are one of the very few machine learning algorithms where we don't need to worry about feature scaling. However, the majority of machine learning and optimization algorithms behave much better if features are on the same scale.\n",
    "\n",
    "Now, there are two common approaches to bringing different features onto the same scale: **normalization** and **standardization**. Those terms are often used quite loosely in different fields, and the meaning has to be derived from the context. \n",
    "\n",
    "* Most often, **normalization** refers to the rescaling of the features to a range of $[0, 1]$, which is a special case of *min-max scaling*. To normalize our data, we can simply apply the min-max scaling to each feature column, where the new value $x_{\\text{norm}}^{(i)}$ of a sample $x^{(i)}$ can be calculated as follows:\n",
    "\n",
    "$$x_{\\text{norm}}^{(i)} = \\frac{x^{(i)} - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}$$\n",
    "\n",
    "Here, $x^{(i)}$ is a particular sample, $x_{\\text{min}}$ is the smallest value in a feature column, and $x_{\\text{max}}$ the largest vaues, respecively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using **standardization**, we center the feature columns at mean 0 with standard deviation 1 so that the feature columns take the form of a normal distribution, which makes it easier to learn the weights. Furthermore, standardization maintains useful information about outliers and makes the algorithm less sensitive to them in contrast to min-max scaling, which scales the data to a limited range of values.\n",
    "\n",
    "$$x_{\\text{std}}^{(i)} = \\frac{x^{(i)} - \\mu_x}{\\sigma_x}$$\n",
    "\n",
    "Here, $\\mu_x$ is the sample mean of a particular feature column and $\\sigma_x$ the corresponding standard deviation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample | Normalization  | Standardization')\n",
    "for i in range(10):\n",
    "    print(str(i) + '      | '+ str(X_train_norm[i,0])+ ' | '+ str(X_train_std[i,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is also important to highlight that we fit the StandardScaler **only once** on the training data and use those parameters to transform the test set or any new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting meaningful features\n",
    "\n",
    "If we notice that a model performs much better on a training dataset than on the test dataset, this observation is a strong indicator for overfitting. Overfitting means that model fits the parameters too closely to the particular observations in the training dataset but does not generalize well to real data—we say that the model has a high variance. A reason for overfitting is that our model is too complex for the given training data and common solutions to reduce the generalization error are listed\n",
    "as follows:\n",
    "\n",
    "* Collect more training data\n",
    "\n",
    "* Introduce a penalty for complexity via regularization\n",
    "\n",
    "* Choose a simpler model with fewer parameters\n",
    "\n",
    "* Reduce the dimensionality of the data\n",
    "\n",
    "Let's look at common ways to reduce overfitting by regularization and dimensionality reduction via feature selection. In particular:\n",
    "\n",
    "1. Sparse solutions with L1 regularization, and\n",
    "\n",
    "2. Sequential feature selection algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sparse solutions with L1 regularization\n",
    "\n",
    "Recall our Linear Regression problem where we used the **L2 regularization** approach to reduce the complexity of a model by penalizing large individual weights, where we defined the L2 norm of our weight vector $\\mathbf{w}$ as follows:\n",
    "\n",
    "$$\\text{L2: }\\Vert\\mathbf{w}\\Vert_2^2 = \\sum_{j=0}^M w_j^2$$\n",
    "\n",
    "Another approach to reduce the model complexity is the related **L1 regularization**:\n",
    "\n",
    "$$\\text{L1: }\\Vert\\mathbf{w}\\Vert_1 = \\sum_{j=0}^M |w_j|$$\n",
    "\n",
    "In contrast to L2 regularization, L1 regularization yields sparse feature vectors; most feature weights will be zero. Sparsity can be useful in practice if we have a high-dimensional dataset with many features that are irrelevant, especially cases where we have more irrelevant dimensions than samples. In this sense, L1 regularization can be understood as a technique for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression(penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l1', C=0.1, multi_class='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both training and test accuracies (both 98 percent) do not indicate any overfitting of our model. When we access the intercept terms via the lr.intercept_ attribute, we can see that the array returns three values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we the fit the LogisticRegression object on a multiclass dataset, it uses the One-vs-Rest (OvR) approach by default where the first intercept belongs to the model that fits class 1 versus class 2 and 3; the second value is the intercept of the model that fits class 2 versus class 1 and 3; and the third value is the intercept of the model that fits class 3 versus class 1 and 2, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight array that we accessed via the lr.coef_ attribute contains three rows of weight coefficients, one weight vector for each class. Each row consists of 13 weights where each weight is multiplied by the respective feature in the 13-dimensional Wine dataset to calculate the net input:\n",
    "\n",
    "$$\\mathbf{y} = w_0x_0 + w_1x_1 + \\cdots + w_M x_M = \\sum_{j=0}^M x_j w_j = \\mathbf{w}^T\\mathbf{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the weight vectors are sparse, which means that they only have a few non-zero entries. As a result of the L1 regularization, which serves as a method for feature selection, we just trained a model that is robust to the potentially irrelevant features in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = plt.subplot(111)\n",
    "colors = ['blue', 'green', 'red', 'cyan', 'magenta',\n",
    "          'yellow', 'black','pink', 'lightgreen',\n",
    "          'lightblue','gray', 'indigo', 'orange']\n",
    "\n",
    "weights, params = [], []\n",
    "\n",
    "for c in np.arange(-4, 6):\n",
    "    lr = LogisticRegression(penalty='l1', C=10.0**c, random_state=0, solver='liblinear', multi_class='auto')\n",
    "    lr.fit(X_train_std, y_train)\n",
    "    weights.append(lr.coef_[1])\n",
    "    params.append(10.0**c)\n",
    "    w = np.array(weights)\n",
    "    \n",
    "for column, color in zip(range(w.shape[1]), colors):\n",
    "    plt.plot(params, w[:, column], label=df_wine.columns[column+1],color=color)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=3)\n",
    "plt.xlim([10.0**(-5), 10.0**5])\n",
    "plt.ylabel('weight coefficient')\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='upper left',)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.38, 1.03), ncol=1, fancybox=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot provides us with further insights about the behavior of L1 regularization. As we can see, all features weights will be zero if we penalize the model with a strong regularization parameter ($C < 0.1$); $C$ is the inverse of the regularization parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sequential feature selection algorithms\n",
    "\n",
    "An alternative way to reduce the complexity of the model and avoid overfitting is dimensionality reduction via feature selection, which is especially useful for unregularized models. There are two main categories of dimensionality reduction techniques: feature selection and feature extraction. \n",
    "\n",
    "* Using feature selection, we select a subset of the original features. \n",
    "* In feature extraction, we derive information from the feature set to construct a new feature subspace.\n",
    "\n",
    "Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial $d$-dimensional feature space to a $k$-dimensional feature subspace where $k<d$. The motivation behind feature selection algorithms is to automatically select a subset of features that are most relevant to the problem to improve computational efficiency or reduce the generalization error of the model by removing irrelevant features or noise, which can be useful for algorithms that don't support regularization. \n",
    "\n",
    "A classic sequential feature selection algorithm is **Sequential Backward Selection (SBS)**, which aims to reduce the dimensionality of the initial feature subspace with a minimum decay in performance of the classifier to improve upon computational efficiency. In certain cases, SBS can even improve the predictive power of the model if a model suffers from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind the SBS algorithm is quite simple: SBS sequentially removes features from the full feature subset until the new feature subspace contains the desired number of features. In order to determine which feature is to be removed at each stage, we need to define criterion function $J$ that we want to minimize. The criterion calculated by the criterion function can simply be the difference in performance of the classifier after and before the removal of a particular feature. Then the feature to be removed at each stage can simply be defined as the feature that maximizes this criterion; or, in more intuitive terms, at each stage we eliminate the feature that causes the least performance loss after removal. Based on the preceding definition of SBS, we can outline the algorithm in 4 simple steps:\n",
    "\n",
    "1. Initialize the algorithm with $k=d$, where $d$ is the dimensionality of the full feature space $\\mathbf{X}_d$.\n",
    "\n",
    "2. Determine the feature $x^-$ that maximizes the criterion $x^- = \\arg\\max J(\\mathbf{X}_k - x^-)$ where $x \\in \\mathbf{X}_k$.\n",
    "\n",
    "3. Remove the feature $x^-$ from the feature set.\n",
    "\n",
    "4. Terminate if $k$ equals the number of desired features, if not, go to step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the SBS algorithm is not implemented in scikit-learn, yet. But since it is so simple, let's go ahead and implement it in Python from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: \"Python Machine Learning\" by Sebastian Raschka\n",
    "\n",
    "class SBS():\n",
    "    def __init__(self, estimator, k_features,\n",
    "        scoring=accuracy_score,test_size=0.25, random_state=1):\n",
    "        self.scoring = scoring\n",
    "        self.estimator = clone(estimator)\n",
    "        self.k_features = k_features\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=self.test_size, random_state=self.random_state)\n",
    "        dim = X_train.shape[1]\n",
    "        self.indices_ = tuple(range(dim))\n",
    "        self.subsets_ = [self.indices_]\n",
    "        score = self._calc_score(X_train, y_train,\n",
    "                                 X_test, y_test, self.indices_)\n",
    "        self.scores_ = [score]\n",
    "        \n",
    "        while dim > self.k_features:\n",
    "            scores = []\n",
    "            subsets = []\n",
    "            for p in combinations(self.indices_, r=dim-1):\n",
    "                score = self._calc_score(X_train, y_train, X_test, y_test, p)\n",
    "                scores.append(score)\n",
    "                subsets.append(p)\n",
    "            best = np.argmax(scores)\n",
    "            self.indices_ = subsets[best]\n",
    "            self.subsets_.append(self.indices_)\n",
    "            dim -= 1\n",
    "            self.scores_.append(scores[best])\n",
    "        self.k_score_ = self.scores_[-1]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[:, self.indices_]\n",
    "    \n",
    "    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n",
    "        self.estimator.fit(X_train[:, indices], y_train)\n",
    "        y_pred = self.estimator.predict(X_test[:, indices])\n",
    "        score = self.scoring(y_test, y_pred)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see our SBS implementation in action using the K-NN classifier from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "sbs = SBS(knn, k_features=1)\n",
    "\n",
    "sbs.fit(X_train_std, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_feat = [len(k) for k in sbs.subsets_]\n",
    "\n",
    "plt.plot(k_feat, sbs.scores_, marker='o')\n",
    "plt.ylim([0.7, 1.1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the following plot, the accuracy of the KNN classifier improved on the validation dataset as we reduced the number of features, which is likely due to a decrease of the **curse of dimensionality**.\n",
    "\n",
    "Also, we can see in the following plot that the classifier achieved 100 percent accuracy for $k=\\{5, 6, 7, 8, 9, 10, 11\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's evaluate the performance of the KNN classifier on the original test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train_std, y_train)\n",
    "\n",
    "print('Training accuracy:', knn.score(X_train_std, y_train))\n",
    "print('Test accuracy:', knn.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding code, we used the complete feature set and obtained $\\sim 98.4$ percent accuracy on the training dataset. However, the accuracy on the test dataset was slightly lower ($\\sim 94.4$ percent), which is an indicator of a slight degree of overfitting. Now let's use the selected 5-feature subset and see how well K-NN performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train_std[:, k5], y_train)\n",
    "\n",
    "print('Training accuracy:', knn.score(X_train_std[:, k5], y_train))\n",
    "print('Test accuracy:', knn.score(X_test_std[:, k5], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using fewer than half of the original features in the Wine dataset, the prediction accuracy on the test set improved by almost 2 percent. Also, we reduced overfitting, which we can tell from the small gap between test ($\\sim 96.3$ percent) and training\n",
    "($\\sim 96.0$ percent) accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Feature selection algorithms in scikit-learn</b>\n",
    "\n",
    "There are many more feature selection algorithms available via scikit-learn. Those include recursive backward elimination based on feature weights, tree-based methods to select features by importance, and univariate statistical tests. A comprehensive discussion of the different feature selection methods is beyond the scope of this book, but a good summary with illustrative examples can be found [here](http://scikit-learn.org/stable/modules/feature_selection.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Assignment\n",
    "\n",
    "* Chapter 4 **\"Building Good Training Sets: Data Preprocessing\"** from Python Machine Learning textbook\n",
    "\n",
    "* Chapter 6 **\"Learning Best Practices for Model Evaluation and Hyperparameter Tuning\"** from Python Machine Learning textbook\n",
    "\n",
    "* Section 5.3 **\"Hyperparameters and Model Validation\"** from Python Data Science Handbook textbook, available online [here](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html)\n",
    "\n",
    "* Section 5.4 **\"Feature Engineering\"** from Python Data Science Handbook textbook, available online [here](https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
