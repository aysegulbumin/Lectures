{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8 - Feature Selection (continued) & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine.columns = ['Class label', 'Alcohol','Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium','Total phenols', 'Flavanoids',\n",
    "                   'Nonflavanoid phenols','Proanthocyanins','Color intensity', 'Hue',\n",
    "                   'OD280/OD315 of diluted wines','Proline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Class labels ', np.unique(df_wine['Class label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df_wine.iloc[:,1:].values, df_wine.iloc[:,0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train)\n",
    "X_test_norm = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "## Selecting meaningful features\n",
    "\n",
    "If we notice that a model performs much better on a training dataset than on the test dataset, this observation is a strong indicator for overfitting. Overfitting means that model fits the parameters too closely to the particular observations in the training dataset but does not generalize well to real dataâ€”we say that the model has a high variance. A reason for overfitting is that our model is too complex for the given training data and common solutions to reduce the generalization error are listed\n",
    "as follows:\n",
    "\n",
    "* Collect more training data\n",
    "\n",
    "* Introduce a penalty for complexity via regularization\n",
    "\n",
    "* Choose a simpler model with fewer parameters\n",
    "\n",
    "* Reduce the dimensionality of the data\n",
    "\n",
    "Let's look at common ways to reduce overfitting by regularization and dimensionality reduction via feature selection. In particular:\n",
    "\n",
    "1. Sparse solutions with L1 regularization, and\n",
    "\n",
    "2. Sequential feature selection algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sparse solutions with L1 regularization\n",
    "\n",
    "Recall our Linear Regression problem where we used the **L2 regularization** approach to reduce the complexity of a model by penalizing large individual weights, where we defined the L2 norm of our weight vector $\\mathbf{w}$ as follows:\n",
    "\n",
    "$$\\text{L2: }\\Vert\\mathbf{w}\\Vert_2^2 = \\sum_{j=0}^M w_j^2$$\n",
    "\n",
    "Another approach to reduce the model complexity is the related **L1 regularization**:\n",
    "\n",
    "$$\\text{L1: }\\Vert\\mathbf{w}\\Vert_1 = \\sum_{j=0}^M |w_j|$$\n",
    "\n",
    "In contrast to L2 regularization, L1 regularization yields sparse feature vectors; most feature weights will be zero. Sparsity can be useful in practice if we have a high-dimensional dataset with many features that are irrelevant, especially cases where we have more irrelevant dimensions than samples. In this sense, L1 regularization can be understood as a technique for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l1', C=0.1, multi_class='auto')\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "print('Training accuracy: ',lr.score(X_train_std, y_train))\n",
    "print('Test accuracy: ',lr.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = plt.subplot(111)\n",
    "colors = ['blue', 'green', 'red', 'cyan', 'magenta',\n",
    "          'yellow', 'black','pink', 'lightgreen',\n",
    "          'lightblue','gray', 'indigo', 'orange']\n",
    "\n",
    "weights, params = [], []\n",
    "\n",
    "for c in np.arange(-4, 6):\n",
    "    lr = LogisticRegression(penalty='l1', C=10.0**c, random_state=0, solver='liblinear', multi_class='auto')\n",
    "    lr.fit(X_train_std, y_train)\n",
    "    weights.append(lr.coef_[1])\n",
    "    params.append(10.0**c)\n",
    "    w = np.array(weights)\n",
    "    \n",
    "for column, color in zip(range(w.shape[1]), colors):\n",
    "    plt.plot(params, w[:, column], label=df_wine.columns[column+1],color=color)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=3)\n",
    "plt.xlim([10.0**(-5), 10.0**5])\n",
    "plt.ylabel('weight coefficient')\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='upper left',)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.38, 1.03), ncol=1, fancybox=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sequential feature selection algorithms\n",
    "\n",
    "An alternative way to reduce the complexity of the model and avoid overfitting is dimensionality reduction via feature selection, which is especially useful for unregularized models. There are two main categories of dimensionality reduction techniques: feature selection and feature extraction. \n",
    "\n",
    "* Using feature selection, we select a subset of the original features. \n",
    "* In feature extraction, we derive information from the feature set to construct a new feature subspace.\n",
    "\n",
    "Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial $d$-dimensional feature space to a $k$-dimensional feature subspace where $k<d$. The motivation behind feature selection algorithms is to automatically select a subset of features that are most relevant to the problem to improve computational efficiency or reduce the generalization error of the model by removing irrelevant features or noise, which can be useful for algorithms that don't support regularization. \n",
    "\n",
    "A classic sequential feature selection algorithm is **Sequential Backward Selection (SBS)**, which aims to reduce the dimensionality of the initial feature subspace with a minimum decay in performance of the classifier to improve upon computational efficiency. In certain cases, SBS can even improve the predictive power of the model if a model suffers from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind the SBS algorithm is quite simple: SBS sequentially removes features from the full feature subset until the new feature subspace contains the desired number of features. In order to determine which feature is to be removed at each stage, we need to define criterion function $J$ that we want to minimize. The criterion calculated by the criterion function can simply be the difference in performance of the classifier after and before the removal of a particular feature. Then the feature to be removed at each stage can simply be defined as the feature that maximizes this criterion; or, in more intuitive terms, at each stage we eliminate the feature that causes the least performance loss after removal. Based on the preceding definition of SBS, we can outline the algorithm in 4 simple steps:\n",
    "\n",
    "1. Initialize the algorithm with $k=d$, where $d$ is the dimensionality of the full feature space $\\mathbf{X}_d$.\n",
    "\n",
    "2. Determine the feature $x^-$ that maximizes the criterion $x^- = \\arg\\max J(\\mathbf{X}_k - x^-)$ where $x \\in \\mathbf{X}_k$.\n",
    "\n",
    "3. Remove the feature $x^-$ from the feature set.\n",
    "\n",
    "4. Terminate if $k$ equals the number of desired features, if not, go to step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the SBS algorithm is not implemented in scikit-learn, yet. But since it is so simple, let's go ahead and implement it in Python from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source code: \"Python Machine Learning\" by Sebastian Raschka\n",
    "\n",
    "class SBS():\n",
    "    def __init__(self, estimator, k_features,\n",
    "        scoring=accuracy_score,test_size=0.25, random_state=1):\n",
    "        self.scoring = scoring\n",
    "        self.estimator = clone(estimator)\n",
    "        self.k_features = k_features\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=self.test_size, random_state=self.random_state)\n",
    "        dim = X_train.shape[1]\n",
    "        self.indices_ = tuple(range(dim))\n",
    "        self.subsets_ = [self.indices_]\n",
    "        score = self._calc_score(X_train, y_train,\n",
    "                                 X_test, y_test, self.indices_)\n",
    "        self.scores_ = [score]\n",
    "        \n",
    "        while dim > self.k_features:\n",
    "            scores = []\n",
    "            subsets = []\n",
    "            for p in combinations(self.indices_, r=dim-1):\n",
    "                score = self._calc_score(X_train, y_train, X_test, y_test, p)\n",
    "                scores.append(score)\n",
    "                subsets.append(p)\n",
    "            best = np.argmax(scores)\n",
    "            self.indices_ = subsets[best]\n",
    "            self.subsets_.append(self.indices_)\n",
    "            dim -= 1\n",
    "            self.scores_.append(scores[best])\n",
    "        self.k_score_ = self.scores_[-1]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[:, self.indices_]\n",
    "    \n",
    "    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n",
    "        self.estimator.fit(X_train[:, indices], y_train)\n",
    "        y_pred = self.estimator.predict(X_test[:, indices])\n",
    "        score = self.scoring(y_test, y_pred)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see our SBS implementation in action using the K-NN classifier from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "sbs = SBS(knn, k_features=1)\n",
    "\n",
    "sbs.fit(X_train_std, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_feat = [len(k) for k in sbs.subsets_]\n",
    "\n",
    "plt.plot(k_feat, sbs.scores_, marker='o')\n",
    "plt.ylim([0.7, 1.1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the following plot, the accuracy of the KNN classifier improved on the validation dataset as we reduced the number of features, which is likely due to a decrease of the **curse of dimensionality**.\n",
    "\n",
    "Also, we can see in the following plot that the classifier achieved 100 percent accuracy for $k=\\{5, 6, 7, 8, 9, 10, 11\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k5 = list(sbs.subsets_[6])\n",
    "print(df_wine.columns[1:][k5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's evaluate the performance of the KNN classifier on the original test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train_std, y_train)\n",
    "\n",
    "print('Training accuracy:', knn.score(X_train_std, y_train))\n",
    "print('Test accuracy:', knn.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding code, we used the complete feature set and obtained $\\sim 96.0$ percent accuracy on the training dataset. However, the accuracy on the test dataset was slightly lower ($\\sim 96.3$ percent), which is an indicator of a slight degree of overfitting. Now let's use the selected 5-feature subset and see how well K-NN performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train_std[:, k5], y_train)\n",
    "\n",
    "print('Training accuracy:', knn.score(X_train_std[:, k5], y_train))\n",
    "print('Test accuracy:', knn.score(X_test_std[:, k5], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using fewer than half of the original features in the Wine dataset, the prediction accuracy on the test set improved by almost 2 percent. Also, we reduced overfitting, which we can tell from the small gap between test ($\\sim 96.3$ percent) and training\n",
    "($\\sim 96.0$ percent) accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Feature selection algorithms in scikit-learn</b>\n",
    "\n",
    "There are many more feature selection algorithms available via scikit-learn. Those include recursive backward elimination based on feature weights, tree-based methods to select features by importance, and univariate statistical tests. A comprehensive discussion of the different feature selection methods is beyond the scope of this book, but a good summary with illustrative examples can be found [here](http://scikit-learn.org/stable/modules/feature_selection.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "An alternative approach to feature selection for dimensionality reduction is **feature extraction**. We will learn about a couple of fundamental techniques that will help us to summarize the information content of a dataset by transforming it onto a new feature subspace of lower dimensionality than the original one:\n",
    "\n",
    "* **Principal Component Analysis (PCA)** for unsupervised data compression\n",
    "\n",
    "* **Linear Discriminant Analysis (LDA)** as a supervised dimensionality reduction technique for maximizing class separability\n",
    "\n",
    "Data compression is an important topic in machine learning, and it helps us to store and analyze the increasing amounts of data that are produced and collected in the modern age of technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Principal component analysis (PCA) is an unsupervised linear transformation technique that is widely used across different fields, most prominently for dimensionality reduction.\n",
    "\n",
    "* PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions that the original one. \n",
    "\n",
    "The orthogonal axes (**principal components**) of the new subspace can be interpreted as the directions of maximum variance given the constraint that the new feature axes are orthogonal to each other.\n",
    "\n",
    "* PCA takes data from *sensor coordinates* to *data centric coordinates* using linear transformations.\n",
    "\n",
    "* PCA uses a **linear transformation** to **minimize the redundancy** of the resulting transformed data (by ending up with data that is uncorrelated).\n",
    "    * This means that every transformed dimension is more informative.\n",
    "    * In this approach, the dimensionality of the space is still the same as the original data, but the space of features are now arranged such that they contain the most information.\n",
    "\n",
    "* If we wish to reduce dimensionality of our feature space, we can choose only the features that carry over the most information in the linearly transformed space.\n",
    "    * In other words, PCA will find the underlying **linear manifold** that the data is embedded in.\n",
    "\n",
    "* **PCA finds the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the data $X$ comprised on $N$ data samples in a D-dimensional space, so $X$ is a matrix of size $D\\times N$.\n",
    "\n",
    "The **first step** in PCA is to centralize or demean $X$.\n",
    "\n",
    "* Without loss of generality, let's assume that we subtracted the mean to the input data, $X$. Now, $X$ has zero mean.\n",
    "\n",
    "The **second step** is to find the linear transformation $A$ that transforms $X$ to a space where features are:\n",
    "1. Uncorrelated (preserve all dimensions)\n",
    "2. reduced (dimensionality reduction)\n",
    "\n",
    "$$Y = AX$$\n",
    "\n",
    "where $A$ is a $D\\times D$ matrix, $X$ is a $D\\times N$ data matrix and therefore $Y$ is also a $D\\times N$ transformed data matrix.\n",
    "\n",
    "The variance of the transformed data $Y$ is given by:\n",
    "\n",
    "\\begin{align}\n",
    "R_y &= E[YY^T] \\\\\n",
    "&= E[AX(AX)^T] \\\\\n",
    "&= E[AXX^TA^T] \\\\\n",
    "&= AE[XX^T]A^T \\\\\n",
    "&= AR_xA^T\n",
    "\\end{align}\n",
    "\n",
    "Note that we are computing the variance along the dimensions of $Y$, therefore, $R_y$ is a $D\\times D$ matrix. Similarly, $R_X$ represents the covariance of the data $X$. Covariances matrices are symmetric therefore $R_X=R_X^T$ and $R_Y=R_Y^T$.\n",
    "\n",
    "Similarly, $R_X$ represents the covariance of the data $X$. \n",
    "\n",
    "If we write $A$ in terms of vector elements:\n",
    "\n",
    "$$A = \\left[\\begin{array}{cc}\n",
    "a_{11} & a_{12}\\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "\\overrightarrow{a_{1}}\\\\\n",
    "\\overrightarrow{a_{2}}\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "Then,\n",
    "\n",
    "\\begin{align}\n",
    "R_y &= \\left[\\begin{array}{c}a_{1}\\\\a_{2}\\end{array}\\right] R_X \\left[\\begin{array}{cc}a_{1} & a_{2}\\end{array}\\right] \\\\\n",
    "&= \\left[\\begin{array}{cc} a_1 R_X a_1^T & a_1 R_X a_2^T\\\\ a_2 R_X a_1^T & a_2 R_X a_2^T\\end{array}\\right]\n",
    "\\end{align}\n",
    "\n",
    "* If we want to represent the data in a space in which the features are **uncorrelated**, what shape does the covariance matrix have to take?\n",
    "\n",
    "Diagonal! Why?\n",
    "\n",
    "* Can we use the eigenvectors of $R_X$ as our linear transformation $A$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case where we are trying to project the data $X$ into a 1-dimensional space, so we are trying to find the direction $a_1$ where maximal data variance is preserved:\n",
    "\n",
    "$$\\arg_{a_1} \\max a_1 R_X a_1^T$$\n",
    "\n",
    "We want this solution to be bounded (considering $a_1 = \\infty$ would maximize), so we need to constraint the vector to have norm 1\n",
    "\n",
    "$$\\Vert a_1\\Vert_2^2 = 1 \\iff a_1 a_1^T = 1$$\n",
    "\n",
    "Then, we using Lagrange Optimization:\n",
    "\n",
    "$$\\mathcal{L} = a_1 R_X a_1^T + \\lambda_1 (1-a_1 a_1^T)$$\n",
    "\n",
    "Solving for $a_1$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a_1} &= 0 \\\\\n",
    "R_X a_1^T + R_X^T a_1^T - 2\\lambda_1 a_1^T &= 0 \\\\\n",
    "R_X a_1^T = \\lambda a_1^T\n",
    "\\end{align}\n",
    "\n",
    "* Does this look familiar?\n",
    "\n",
    "This is stating that $a_1^T$ must be an eigenvector of $R_X$!\n",
    "\n",
    "So coming back to the question \"Can we use the eigenvectors of $X$ as our linear transformation $A$?\" YES!\n",
    "\n",
    "* If we left multiply by $a_1$ and make use of $a_1a_1^T = 1$:\n",
    "\n",
    "$$a_1 R_X a_1^T = \\lambda_1$$\n",
    "\n",
    "**So the variance will be maximum when we set the project direction $a_1$ equal to the eigenvector having the largest eigenvalue $\\lambda_1$**.\n",
    "\n",
    "* This eigenvector is known as the firt **principal component**.\n",
    "\n",
    "* As you may anticipate, the linear trasnformation $A$ will the be a matrix whose row entries are **sorted** the eigenvectors (sorted by their correspondent eigenvalue).\n",
    "\n",
    "$$A = \\left[\\begin{array}{c}\n",
    "\\overrightarrow{a_{1}}\\\\\n",
    "\\overrightarrow{a_{2}}\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "where $a_1$ and $a_2$ are eigenvectors of $R_X$ with correspondent eigenvalues $\\lambda_1$ and $\\lambda_2$ with $\\lambda_1>\\lambda_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps of PCA\n",
    "\n",
    "Consider the data $X$:\n",
    "\n",
    "1. Subtract the mean $\\mu = \\frac{1}{N}\\sum_{i=1}^N x_i$\n",
    "\n",
    "2. Compute the covariance matrix $R_X$\n",
    "\n",
    "3. Compute eigenvectors and eigenvalues of the matrix $R_X$. Tip: sort the eigenvectors ($e_i$) in decreasing eigenvalue ($\\lambda_i$) order.\n",
    "\n",
    "4. The linear transformation $A$ will be: $A = \\left[\\begin{array}{c} \\overrightarrow{u_{1}}\\\\ \\overrightarrow{u_{2}} \\end{array}\\right]$, where $\\lambda_1 > \\lambda_2$\n",
    "\n",
    "5. Transformed data: $y=Ax$\n",
    "\n",
    "Note that the formal definition of covariance already accounts for demeaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X, m, display=1):\n",
    "    '''This function implements PCA. The data matrix X is DxN matrix, \n",
    "    where D is the dimension and N the number of points'''\n",
    "    \n",
    "    D,N = X.shape\n",
    "    \n",
    "    # Demean the Data\n",
    "    data = X - X.mean(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    # Covariance of the input data X\n",
    "    cov_mat = np.cov(data)\n",
    "    if display:\n",
    "        sns.heatmap(cov_mat)\n",
    "        plt.title('Covariance Matrix')\n",
    "        plt.show();\n",
    "    \n",
    "    # Find eigenvectors and eigenvalues \n",
    "    eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "    if display:\n",
    "        print(eigen_vals)\n",
    "    \n",
    "    # Sort eigenvectors by magnitude of eigenvalues\n",
    "    eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) for i in range(len(eigen_vals))]\n",
    "    eigen_pairs = sorted(eigen_pairs, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Linear transformation\n",
    "    A = np.hstack(([eigen_pairs[i][1][:, np.newaxis] for i in range(m)]))\n",
    "    \n",
    "    #compute explained variance and plot\n",
    "    if display:\n",
    "        total = sum(eigen_vals)\n",
    "        var_explained = [(i/total) for i in sorted(eigen_vals, reverse=True)]\n",
    "        cum_var_exp = np.cumsum(var_explained)\n",
    "        plt.bar(range(1,D+1), var_explained, alpha=0.5, align='center', label='individual explained variance')\n",
    "        plt.step(range(1,D+1), cum_var_exp, alpha=0.5, where='mid', label='cumulative explained variance')\n",
    "        plt.ylabel('Explained variance ratio')\n",
    "        plt.xlabel('Principal components')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show();\n",
    "    return A, eigen_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, eigen_pairs = PCA(X_train_std.T, 13, display=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot indicates that the first principal component alone accounts for 40 percent of the variance. Also, we can see that the first two principal components combined explain almost 60 percent of the variance in the data.\n",
    "\n",
    "Although the explained variance plot reminds us of the feature importance, we shall remind ourselves that PCA is an unsupervised method, which means that information about the class labels is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = X_train_std@A\n",
    "\n",
    "print(X_train_std.shape, X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = np.cov(X_train_pca.T)\n",
    "\n",
    "sns.heatmap(cov_mat);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_pca[y_train==l, 0], X_train_pca[y_train==l, 1],c=c, label=l, marker=m)\n",
    "\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: MNIST Dataset Handwritten Digits\n",
    "\n",
    "(http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(images, labels), (images_test, labels_test) = mnist.load_data()\n",
    "\n",
    "images = images/255.\n",
    "images_test = images_test/255.\n",
    "print(images.shape,labels.shape,images_test.shape,labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6, 8, subplot_kw=dict(xticks=[], yticks=[]))\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(images[1250 * i,:,:], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening image into vector\n",
    "N,D,_ = images.shape\n",
    "Ntest,D,_ = images_test.shape\n",
    "\n",
    "X = images.flatten().reshape(N, D*D)\n",
    "X_test = images_test.flatten().reshape(Ntest, D*D)\n",
    "\n",
    "print(X.shape, labels.shape, X_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, eigen_pairs = PCA(X.T,2,0)\n",
    "\n",
    "y = X@A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.shape,X.shape,y.shape,labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.scatter(y[:, 0], y[:, 1], c=labels, cmap=plt.cm.get_cmap('jet', 10))\n",
    "plt.colorbar(ticks=range(10))\n",
    "plt.clim(-0.5, 9.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.suptitle('MNIST Top 9 Eigenvectors', size=16)\n",
    "for i in range(9):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(np.real(eigen_pairs[i][1].reshape(28,28)),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Eigenfaces\n",
    "\n",
    "* Part of this example comes from: http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.decomposition import PCA as PCA_skl\n",
    "\n",
    "n_row, n_col = 3, 3\n",
    "n_components = n_row * n_col\n",
    "image_shape = (64, 64)\n",
    "\n",
    "# Load faces \n",
    "dataset = fetch_olivetti_faces(shuffle=True)\n",
    "faces = dataset.data\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % n_samples)\n",
    "\n",
    "#Define function to plot imagery\n",
    "def plot_gallery(title, images, n_col=n_col, n_row=n_row):\n",
    "    plt.figure()\n",
    "    plt.suptitle(title, size=16)\n",
    "    for i, comp in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        comp = comp.reshape((64,64))\n",
    "        plt.imshow(comp,cmap='gray')\n",
    "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n",
    "\n",
    "scikit_pca = PCA_skl(n_components = n_components, whiten=True)\n",
    "X_spca = scikit_pca.fit_transform(faces.T)\n",
    "\n",
    "plot_gallery(\"Olivetti faces\", faces[0:n_components,:])\n",
    "plot_gallery('Results', X_spca.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Half-Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples = 100, random_state = 123)\n",
    "plt.scatter(X[y==0,0], X[y==0, 1], color='red', marker = '^', alpha=0.5)\n",
    "plt.scatter(X[y==1,0], X[y==1, 1], color='blue', marker = 'o', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scikit_pca = PCA_skl(n_components = 2)\n",
    "X_spca = scikit_pca.fit_transform(X)\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(7,3))\n",
    "ax[0].scatter(X_spca[y==0,0], X_spca[y==0,1], color='red', marker = '^', alpha=0.5)\n",
    "ax[0].scatter(X_spca[y==1,0], X_spca[y==1,1], color='blue', marker = 'o', alpha=0.5)\n",
    "ax[1].scatter(X_spca[y==0,0], np.zeros((50,1)), color='red', marker = '^', alpha=0.5)\n",
    "ax[1].scatter(X_spca[y==1,0], np.zeros((50,1)), color='blue', marker = 'o', alpha=0.5)\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[1].set_ylim([-1,1])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xlabel('PC1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Assignment\n",
    "\n",
    "* Chapter 5 **\"Compressing Data via Dimensionality Reduction\"** from Python Machine Learning textbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
