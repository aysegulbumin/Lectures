{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 16 - Introduction to Multi-layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Convergence Theorem\n",
    "\n",
    "The *Perceptron convergence theorem* states that for any data set which is linearly separable the perceptron learning algorithm is guaranteed to find a solution in a finite number of steps.\n",
    "\n",
    "* In other words, the Perceptron learning rule is guaranteed to converge to a weight vector that correctly classifies the examples provided the training examples are linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron (MLP)\n",
    "\n",
    "A multilayer perceptron (MLP) is a class of feed-forward artificial neural network (ANN). An MLP consists of at least three layers of nodes. Except for the input nodes, each node is a neuron that uses an activation function (either linear or non-linear). \n",
    "\n",
    "Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.\n",
    "\n",
    "<div><img src=\"figures/mlp2.png\", width=\"500\"><!div>\n",
    "\n",
    "In MLPs, each neuron's output can be subject to different activation functions. The choice of the activation function is in itself a *free-parameter*.\n",
    "* Which activation function would you use if you desired labels are $\\{1,2,3,4,\\dots,8,9\\}$? \n",
    "\n",
    "* *What each of the layers do?*\n",
    "    * The first hidden layer draws boundaries\n",
    "    * The second hidden layer combines the boundaries\n",
    "    * The third and further layers can generate arbitrarily complex shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Approximation Theorem\n",
    "\n",
    "\"The *Universal Approximation Theorem* states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of $\\mathbb{R}^N$, under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters.\" Source: [Wikipedia, 2019](https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n",
    "\n",
    "Let $\\phi(\\cdot)$ be a non-constant, bounded and monotonic-increasing continuous function. Let $I_{m_0}$ denote the $m_0$-dimensional unit hypercube $[0, 1]^{m_0}$. The space of continuous functions on $I_{m_0}$ is denoted by $C(I_{m_0})$.  Then, given any function $f \\ni C(I_{m_0})$ and $\\epsilon > 0$, there exists an integer $m_1$ and sets of real constants $\\alpha_i, \\beta_i,$ and $w_{ij}$, where $i = 1, \\ldots, m_1$ and $j = 1, \\ldots, m_0$ such that we may define\n",
    "\n",
    "$$F(x_1, \\ldots, x_{m_0}) = \\sum_{i=1}^{m_1} \\alpha_i \\phi\\left( \\sum_{j=1}^{m_0} w_{ij}x_j + b_i\\right)$$\n",
    "\n",
    "as an approximation realization of the function $f(\\cdot)$, that is, \n",
    "\n",
    "$$\\left| F(x_1, \\ldots, x_{m_0}) - f(x_1, \\ldots, x_{m_0}) \\right| < \\epsilon$$\n",
    "\n",
    "for all $x_1, x_2, \\ldots, x_{m_0}$ that like in the input space.\n",
    "\n",
    "Essentially, the Universal Approximation Theorem states that a single hidden layer is sufficient for a multilayer perceptron to compute a uniform $\\epsilon$ approximation to a given training set - provided you have the *right* number of neurons and the *right* activation function. \n",
    "\n",
    "* However, this does not say that a single hidden layer is optimal with regards to learning time, generalization, etc.)\n",
    "\n",
    "* In other words, a **feed-forward MLP with one hidden layer can approximate arbitrarily closely any continuous function**. (Wow!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Backpropagation\n",
    "\n",
    "* The learning procedure involves the presentation of a set of pairs of input and output patterns, $X=\\{x_i\\}_{i=1}^N$ and $Y=\\{y_j\\}_{j=1}^M$. The system uses the input vector to produce its own output vector and then compares this with the \\emph{desire output}, or \\emph{target output} $t=\\{t_j\\}_{j=1}^M$. If there is no difference, no learning takes place. Otherwise, the weights are changed to reduce the difference. This procedure is basically the perceptron learning algorithm.\n",
    "\n",
    "* This procedure can be *automated* by the machine itself, without any outside help, if we provide some **feedback** to the machine on how it is doing. The feedback comes in the form of the definition of an *error criterion* or *objective function* that must be *minimized* (e.g. Mean Squared Error). For each training pattern we can define an error ($\\epsilon_k$) between the desired response ($d_k$) and the actual output ($y_k$). Note that when the error is zero, the machine output is equal to the desired response. This learning mechanism is called **(error) backpropagation** (or **BP**).\n",
    "\n",
    "* The backpropagation algorithm consists of two phases: \n",
    "     * **Forward phase:** computes the *functional signal*, feed-forward propagation of input pattern signals through the network.\n",
    "    * **Backward phase:** computes the *error signal*, propagates the error backwards through the network starting at the output units (where the error is the difference between desired and predicted output values).\n",
    "\n",
    "<div><img src=\"figures/ForwardBackward.png\", width=\"300\"><!div>\n",
    "\n",
    "* **Objective function/Error Criterion:** there are many possible definitions of the error, but commonly in neuro-computing one uses the error variance (or power):\n",
    "$$J(w) = \\frac{1}{2}\\sum_{k=1}^N \\epsilon^2 = \\frac{1}{2} \\sum_{k=1}^N (d_k-y_k)^2 = \\frac{1}{2} \\sum_{k=1}^N (d_k-w^Tx_k)^2$$\n",
    "\n",
    "* Now we need to define an **adaptive learning** algorithm. Backpropagation commonly uses the gradient descent as the adaptive learning algorithm.\n",
    "\n",
    "* **Adaptive Learning Algorithm:** there are many learning algorithms, the most common is the method of Gradient/Steepest Descent.\n",
    "    * Move in direction opposite to the gradient vector, $\\nabla E(\\mathbf{w})$\n",
    "\\begin{align}\n",
    "w^{(n+1)} &= w^{(n)} - \\eta \\nabla J(w^{(n)})\\\\\n",
    "\\Delta w^{(n)} &= w^{(n+1)} - w^{(n)}\\\\\n",
    "\\Delta w^{(n)} &= - \\eta \\nabla J(w^{(n)}) \\quad \\text{ Error correction rule }\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "There are many approaches to train a neural network. One of the most commonly used is the **Error Backpropagation Algorithm**. \n",
    "\n",
    "Let's first consider the output layer:\n",
    "* Given a training set, $\\left\\{ x_n, d_n\\right\\}_{n = 1}^N$, we want to find the parameters of our network that minimizes the squared error: \n",
    "$$J(w) = \\frac{1}{2} \\sum_{k=1}^N (d_k - y_k)^2$$\n",
    "\n",
    "* In order to use gradient descent, we need to compute the analytic form of the gradient, $\\frac{\\partial J}{\\partial w_{ij}}$.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Chain Rule</b>\n",
    "\n",
    "Given a labelled training set, $\\left\\{ x_n, d_n\\right\\}_{n = 1}^N$, consider the objetive function \n",
    "\n",
    "$$J(w) = \\frac{1}{2} \\sum_{j=1}^N e_j^2$$ \n",
    "\n",
    "where $w$ are the parameters to be estimated and $\\forall j$: \n",
    "\n",
    "\\begin{align}\n",
    "e_j &= d_j - y_j \\\\\n",
    "y_j &= \\phi(v_j)\\text{, }\\phi(\\bullet)\\text{ is an activation function}\\\\\n",
    "v_j &= w^Tx_j\\text{ (note that }x_j\\in\\mathbb{R}^{D+1}\\text{)}  \n",
    "\\end{align}\n",
    "\n",
    "Using the Chain Rule, we find:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{ij}} = \\frac{\\partial J}{\\partial e_j} \\frac{\\partial e_j}{\\partial y_j} \\frac{\\partial y_j}{\\partial v_j} \\frac{\\partial v_j}{\\partial w_{ij}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial e_j} = \\frac{1}{2} 2 e_j = d_j - y_j$$\n",
    "\n",
    "$$\\frac{\\partial e_j}{\\partial y_j} = -1$$\n",
    "\n",
    "$$\\frac{\\partial y_j}{\\partial v_j} = \\frac{\\partial \\phi(v_j)}{\\partial v_j} = \\phi'(v_j)$$\n",
    "\n",
    "$$\\frac{\\partial v_j}{\\partial w_{ij}} = x_i$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{ij}} = (d_j - y_j) (-1) \\phi'(v_j) x_i$$\n",
    "\n",
    "</div>\n",
    "\n",
    "* If activation function is the sigmoid, $\\phi(x) = \\frac{1}{1+e^{-x}}$, then $\\phi'(x) = \\phi(x)(1-\\phi(x))$\n",
    "\n",
    "* If activation function is the hyperbolic tangent (tanh), $\\phi(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$, then $\\phi'(x) = 1-\\phi(x)^2$\n",
    "\n",
    "* If activation function is the ReLU, $\\phi(x) = \\begin{cases}0, & x \\leq 0 \\\\ x, & x>0 \\end{cases}$, then $\\phi'(x) = \\begin{cases}0, & x \\leq 0 \\\\ 1, & x>0 \\end{cases}$\n",
    "\n",
    "\n",
    "Now that we have the gradient, how do we use this to update the output layer weights in our MLP?\n",
    "\n",
    "$$w_{ij}^{(t+1)} = w_{ij}^{(t)} - \\eta \\frac{\\partial J}{\\partial w_{ij}} = w_{ij}^{(t)} + \\eta (d_j - y_j)\\phi'(v_j) x_i$$\n",
    "\n",
    "\n",
    "* How will this update equation  (for the output layer) change if the network is a multilayer perceptron with hidden units?\n",
    "\n",
    "* Can you write this in vector form to update all weights simultaneously?\n",
    "\n",
    "* Next, the hidden layers..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Reading \n",
    "\n",
    "[\"Learning representations by back-propagating errors\"](https://www.nature.com/articles/323533a0) by David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Nature 323 (6088): 533–536, 8, October 1986."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
