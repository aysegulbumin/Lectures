{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 15 - The Perceptron Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an Artificial Neural Network (ANN or NN), we attempt to model the human neural network and neurons using programming constructs.\n",
    "\n",
    "A basic model for a neuron consists of the following: \n",
    "\n",
    "* A set of *synapses* each of which is characterized by a *weight* (which includes a *bias*).\n",
    "\n",
    "* An *adder*.\n",
    "\n",
    "* An *activation function* (e.g. heaviside function, piece-wise linear function, sigmoid function, ReLu, Leaky ReLu etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Activation Functions\n",
    "\n",
    "In artificial neural networks (ANN), the *activation function* of a neuron defines the output of that neuron given an input or set of inputs.\n",
    "\n",
    "There are many activation functions, the most common are:\n",
    "\n",
    "1. **Heaviside step function:**\n",
    "$$\\phi(x) = \\begin{cases}1, & x >0 \\\\ 0, & x\\leq 0\\end{cases}$$\n",
    "\n",
    "2. **Sigmoid function:**\n",
    "$$\\phi(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "3. **Hyperbolic tangent function (tanh):**\n",
    "$$\\phi(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "4. **Rectified Linear (ReLu) function:**\n",
    "$$\\phi(x) = \\begin{cases} x, & x\\geq 0 \\\\ 0, & x < 0\\end{cases}$$\n",
    "\n",
    "5. **Leaky ReLu function:**\n",
    "$$\\phi(x) = \\begin{cases} x, & x\\geq 0 \\\\ 0.01x, & x < 0\\end{cases}$$\n",
    "\n",
    "There are many many other functions ([Wikipedia link](https://en.wikipedia.org/wiki/Activation_function))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rosenblatt's Perceptron\n",
    "\n",
    "The Reosenblatt's Perceptron models a *single neuron cell* with a heaviside activation function $\\phi(x) = \\begin{cases} 1, & x>0 \\\\ 0, & x\\leq 0\\end{cases}$\n",
    "\n",
    "<div><img src=\"figures/modelneuron.png\", width=\"500\"><!div>\n",
    "\n",
    "We can write this mathematically as: \n",
    "\n",
    "$$y_k = \\varphi\\left( \\sum_{j=1}^m w_{kj}x_j + b_k\\right)$$\n",
    "\n",
    "* *What does this look like graphically?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Learning Algorithm\n",
    "\n",
    "Suppose we have a \"neural network\" made of only one neuron - i.e., **Rosenblatt's perceptron** and we would like to train it to distinguish between two classes; where the activation function is:\n",
    "\n",
    "$$y_i = \\varphi(x_i) = \\begin{cases} 1 & \\text{if }w_{ki}\\mathbf{x}_i+b_k>0\\\\ 0 & \\text{otherwise}\\end{cases}$$\n",
    "\n",
    "For Rosenblatt's perceptron to be effective, the classes must be linearly separable. Mathematically, the classes must satisfy:\n",
    "\n",
    "$$\\exists \\mathbf{w} \\mid \\mathbf{w}^T\\mathbf{x} +b > 0, \\quad \\forall \\mathbf{x} \\in C_1;  \\quad \\mathbf{w}^T\\mathbf{x} +b\\leq 0, \\quad \\forall \\mathbf{x} \\in C_2$$\n",
    "\n",
    "* **The Perceptron Learning Algorithm will converge to the correct solution if the classes are linearly separable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an alternative error function known as the *perceptron criterion*. To derive this, we note that we are seeking a weight vector $\\mathbf{w}$ such that patterns $x_i$ in class $C_1$ will have $\\mathbf{w}^Tx_i + b > 0$, whereas the patterns $x_i$ in class $C_2$ have $\\mathbf{w}^Tx_i + b < 0$. Using the $t\\in \\{-1,1\\}$ target coding scheme it follows that we would like all patterns to satisfy\n",
    "\n",
    "$$(\\mathbf{w}^Tx_i + b)t_i > 0$$\n",
    "\n",
    "* The perceptron criterion associates zero error with any pattern that is correctly classified, whereas for a misclassified pattern $x_i$ it tries to minimize the quantity $-(\\mathbf{w}^Tx_i + b)t_i$.\n",
    "\n",
    "* The perceptron criterion is therefore given by:\n",
    "\n",
    "$$E_p(\\mathbf{w},b) = - \\sum_{n\\in\\mathcal{M}} (\\mathbf{w}^T \\mathbf{x}_n + b) t_n$$\n",
    "\n",
    "where $\\mathcal{M}$ denotes the set of all misclassified patterns.\n",
    "\n",
    "* We now apply the *stochastic gradient descent* algorithm to this error function. The change in the weight vector $\\mathbf{w}$ is then given by:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{w}^{(t+1)} &\\leftarrow & \\mathbf{w}^{(t)} - \\eta \\frac{\\partial E_p(\\mathbf{w},b)}{\\partial \\mathbf{w}} = \\mathbf{w}^{(t)} + \\eta \\mathbf{x}_n t_n\\\\\n",
    "b^{(t+1)} &\\leftarrow & b^{(t)} - \\eta\\frac{\\partial E_p(\\mathbf{w},b)}{\\partial b} = b^{(t)} + \\eta t_n\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\eta$ is the **learning rate** parameter and $t$ is an integer that indexes the iteration steps of the algorithm. \n",
    "\n",
    "* Note that, as the weight vector evolves during training, the set of patterns that are misclassified will change.\n",
    "\n",
    "<div><img src=\"figures/PerceptronLearning.png\", width=\"600\"><!div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def generateMVNRandData(Npts, mu, sigma):\n",
    "    data  = np.random.multivariate_normal(mu, sigma*np.eye(len(mu)), Npts)\n",
    "    return data\n",
    "\n",
    "def plotLine(weights, range):\n",
    "    x = np.array(range)\n",
    "    y = -(weights[0]/weights[1])-(weights[2]/weights[1])*x\n",
    "    plt.plot(y,x)\n",
    "\n",
    "def perceptronLearningAlg(data,labels,eta,nIterations):\n",
    "    nPts = data.shape[0]\n",
    "    weights = np.random.rand(data.shape[1])\n",
    "    print('Initial weights:', weights)\n",
    "    \n",
    "    error = 1\n",
    "    iter = 0\n",
    "    \n",
    "    # TO BE COMPLETED IN CLASS\n",
    "    \n",
    "#                 plt.scatter(data[:,1],data[:,2], c=labels, linewidth=0)\n",
    "#                 plotLine(weights, [-2,2]);\n",
    "#                 plt.pause(2)\n",
    "                \n",
    "    print('Final Iteration: ', iter,'; Final Error: ', error)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Npts  = 100\n",
    "mu1   = [2,2]\n",
    "mu2   = [0,0]\n",
    "var   = .1\n",
    "eta   = 0.3\n",
    "nIterations = 10;\n",
    "\n",
    "data1 = np.array(generateMVNRandData(Npts, mu1, .1))\n",
    "data1 = np.hstack((np.ones((Npts,1)),data1))\n",
    "data2 = np.array(generateMVNRandData(Npts, mu2, .1))\n",
    "data2 = np.hstack((np.ones((Npts,1)),data2))\n",
    "data  = np.vstack(( data1, data2))\n",
    "labels= np.hstack((np.ones(Npts), -np.ones(Npts)))\n",
    "\n",
    "plt.scatter(data[:,1],data[:,2], c=labels, linewidth=0)\n",
    "\n",
    "perceptronLearningAlg(data,labels,eta,nIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food for Thought\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "1. Consider a neuron with two inputs and one output and a step function. If two weights are $w_1=1$ and $w_2 =1$, and the bias is $b=-1.5$, then what is the output for inputs $(0,0)$, $(1,0)$, $(0,1)$, and $(1,1)$?\n",
    "\n",
    "2. How does the learning behavior change with changes in $\\eta$? as $\\eta$ increases? as $\\eta$ decreases?\n",
    "\n",
    "3. How would you generate overlapping classes using the provided code? Explain your answer. (Only change parameters. You do not need to change code.)\n",
    "\n",
    "4. What happens to the learning behavior when you have overlapping classes?\n",
    "\n",
    "5. The implementation provided uses $\\{-1,1\\}$ labels. Suppose we want to use labels $\\{0,1\\}$. How can we formulate the Perceptron Learning? How does the code need to change to account for this difference (i.e., suppose you want to use $\\{0,1\\}$ labels. What would you need to change in the code?) Why?\n",
    "\n",
    "6. In the provided code, there is not a separate line for learning the bias $b$ as in the pseudo-code above. How is it being estimated and represented it in the code? (... the code *is* still learning the bias value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intro to Multilayer Perceptron\n",
    "\n",
    "Suppose you had the following neural network:\n",
    "\n",
    "<div><img src=\"figures/MLP.png\", width=\"300\"><!div>\n",
    "\n",
    "with a heaviside activation function:\n",
    "\n",
    "$$\\phi(x) = \\begin{cases} 1 & x > 0 \\\\ -1 & x \\leq 0 \\end{cases}$$\n",
    "\n",
    "1. What is the expression of the output value $y$ in terms of the input values?\n",
    "\n",
    "2. What is the output with the following input values?\n",
    "    * $[0,0]$\n",
    "    * $[-2, -2.5]$\n",
    "    * $[-5, 5]$\n",
    "    * $[10, 3]$\n",
    "\n",
    "3. What does the decision surface of this network look like graphically? Draw it out by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Trees\n",
    "\n",
    "The following table contains training examples that help predict whether a patient is likely to have a heart attack ($y = 1$ for \"Heart Attack\", $y=0$ for \"NO Heart Attack\").\n",
    "\n",
    "| $x_1$: chest pain?   | $x_2$:  male?        | $x_3$:  smokes?      | $x_4$:  exercises?   | $y$: heart attack?   |\n",
    "|----------------------|----------------------|----------------------|----------------------|----------------------|\n",
    "|          1           |          1           |          0           |          1           |          1           |\n",
    "|          1           |          1           |          1           |          0           |          1           |\n",
    "|          0           |          0           |          1           |          0           |          1           |\n",
    "|          0           |          1           |          0           |          1           |          0           |\n",
    "|          1           |          0           |          1           |          1           |          1           |\n",
    "|          0           |          1           |          1           |          1           |          0           |\n",
    "\n",
    "In the case of any ties, we will prefer to predict class $y = 1$. For the next steps consider $0 \\log_2 0 = 0$.\n",
    "\n",
    "1. Compute the information gain (entropy function) for each feature $x_i$. Which feature should be the root node?\n",
    "\n",
    "2. Draw the complete decision tree that will be learned from this data. Justify every split based on information gain at every node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
