{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - Polynomial Regression\n",
    "\n",
    "Consider the polynomial curve fitting example discussed in class:\n",
    "\n",
    "$$y(x,\\mathbf{w}) = w_0 + w_1 x + w_2 x^2 + \\cdots + w_Mx^M = \\sum_{j=0}^M w_j x^j$$\n",
    "\n",
    "Suppose you have a data set, $X$, that you split into two groups once: a fixed training subset, $X^{\\text{train}}$, and a fixed validation subset $X^{\\text{validation}}$. After fitting the polynomial to the training set across a range of model orders and evaluating on both training and validation sets, you obtain the following plot.\n",
    "\n",
    "<div><img src=\"figures/polynomial.png\", width=\"400\"><!div>\n",
    "\n",
    "Answers the following questions:\n",
    "\n",
    "1. Based on this plot, provide a discussion about which model order, $M$, should be used to avoid overfitting.\n",
    "\n",
    "2. Since you have split your data into training and validation sets, are you safe from overfitting or can you still overfit? Explain your reasoning.\n",
    "\n",
    "3. What are the common approaches to avoid overfitting given a particular model?\n",
    "\n",
    "4. For the data set $X=\\{x_1,x_2,\\dots,x_N\\}$ and the labels $t=\\{t_1,t_2,\\dots,t_N\\}$, what objective function can you use if you want to optimize the polynomial linear regression model with a regularizer on the weights $w=[w_0,w_1,\\dots,w_M]^T$? Write down the equation for this objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Curse of Dimensionality\n",
    "\n",
    "What is the curse of dimensionality? Why does it cause problems for certain Machine Learning algorithms? Describe two approaches to address curse of dimensionality. In your description state why each method is effective and what are its limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 - Curse of Dimensionality \n",
    "\n",
    "Recall our discussion of the volume of the crust, i.e., the case of a sphere $S_2$ of radius $r-\\epsilon$ inscribed within another sphere $S_1$ of radius $r$ and the relative volume of the crust and the outer sphere as we increase dimensionality $D$:\n",
    "\n",
    "$$\\frac{V_{crust}}{V_{S_1}} = \\frac{V_{S_1}-V_{S_2}}{V_{S_1}} = 1 - \\left(1-\\frac{\\epsilon}{r}\\right)^D$$\n",
    "\n",
    "Describe (in paragraph form, be clear and thorough) how this concept relates to number of data points needed during classification as we increase the number of features we use for classification. In your discussion, be sure to answer: (1) the case when feature are uncorrelated, (2) the case when features are strongly correlated (e.g., $f_1 = af_2 + bf_3$), and (3) why this is an important issue in machine learning in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 - Principal Component Analysis\n",
    "\n",
    "Consider the following four two-dimensional data sets each containing two clusters of data points (shown with \"circles\" and \"crosses\"). Suppose you would like to apply Principal Component Analysis (PCA) to reduce the dimensionality of each of these data sets from 2-D to 1-D where the two clusters remain separated in the 1-D projection. For each data set, address each of the following questions:\n",
    "\n",
    "1. Draw the eigenvectors on each figure and clearly identify the (axis) direction of 1-D PCA projection.\n",
    "2. Will PCA be effective at keeping the two clusters separated in the 1-D projection? Why or why not? If yes, state what characteristics of the data set allow PCA to be effective. If no, state what characteristics of the data set cause PCA to fail.\n",
    "3. Can you think of another technique that would be successful at reducing the dimensionality of this data set while maintaining (or increase) separation between the two clusters? State the other method and describe why it would be successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<div><img src=\"figures/set 1.png\", width=\"500\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<div><img src=\"figures/set 2.png\", width=\"500\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<div><img src=\"figures/set 3.png\", width=\"500\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<div><img src=\"figures/set 4.png\", width=\"500\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 - Principal Component Analysis\n",
    "\n",
    "Suppose $X$ is a zero-mean data set of size $2\\times N$ with covariance matrix $R_x = E[XX^T]= \\left[\\begin{array}{cc} 1 & 0\\\\ 1 & 3 \\end{array}\\right]$.\n",
    "\n",
    "\n",
    "1. Write down the eigenvalue/eigenvector equation.\n",
    "\n",
    "2. Calculate the eigenvalues $\\lambda_1$ and $\\lambda_2$ associated with the eigenvectors $u_1 = \\left[\\begin{array}{c} 2\\\\ -1 \\end{array}\\right]$ and $u_2 = \\left[\\begin{array}{c} 0\\\\ 1 \\end{array}\\right]$, respectively, of $R_x$.\n",
    "\n",
    "3. Suppose $Y$ is the PCA transformation of $X$. Write the formula for computing $Y$ from $X$. Be as specific as possible, you may use information from earlier parts.\n",
    "\n",
    "4. What is the covariance matrix of $Y$? Why? Be as specific as possible, you may use information from earlier parts.\n",
    "\n",
    "5. What is the amount of explained variance of the 1-D PCA projection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6 - Cross-Validation\n",
    "\n",
    "Suppose you have 100 training samples that you are using to train a classifier to distinguish between four classes. The training data has 50 samples of class 1, 25 samples of class 2, 20 samples of class 3 and 5 samples of class 4. To evaluate the stability and performance of your classifier on each class, you use 10-fold cross-validation. Is it a good strategy to randomly partition the data into 10 folds? Why or why not? If yes, fully justify why. If no, state why not, provide an alternate cross-validation scheme and justify the new scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7 - Regularization\n",
    "\n",
    "Suppose you would like to perform feature selection by beginning with too many extracted features and, then, driving the weights for unnecessary features to zero through the use of an appropriate regularization term. Considering the following two regularization terms,\n",
    "\n",
    "$$ E_{c,1} = \\sum_{i=1}^M w_i^2$$\n",
    "\n",
    "$$ E_{c,2} = \\sum_{i=1}^M |w_i|$$\n",
    "\n",
    "which would be more effective for driving the weights to zero? Why? Clearly explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 8 - ROC Curves and Confusion Matrices\n",
    "\n",
    "Suppose you have the following training data set (associated with data collected during 1 minute of a detection system):\n",
    "\n",
    "$$ X = \\{(1,1,2); (10,3,0); (-5,-4,1); (2,-3,1); (10,10,20); (0,0,0)\\}$$\n",
    "\n",
    "$$y = \\{-1,1,1,-1,-1,1\\}$$\n",
    "\n",
    "where $y_i=1$ indicates a true target and $y_i = -1$ indicates a non-target data point. Suppose you trained a classifier to produce a confidence of target given a sample. For the above data points, your MLP produced the following confidence values:\n",
    "\n",
    "$$ c = \\{.7, .6, .2, .3, 0, .9\\}$$\n",
    "\n",
    "1. Draw the associated ROC curve.\n",
    "\n",
    "2. To make a final decision, suppose you thresholded at a confidence value of 0.5 where everything $\\leq 0.5$ is marked as non-target and $> 0.5$ is marked as target. What would be your resulting confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
